import os
import uvicorn
import json
import asyncio
import re
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm

from fastapi import Depends, FastAPI, UploadFile, File, Form, status, Header, Request, Body, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi import HTTPException
import pandas as pd
import traceback
import io
import math
import uuid
import httpx
import numpy as np
# --- Importaciones de nuestros nuevos mÃ³dulos ---
from firebase_admin import firestore
from google.cloud.firestore_v1.base_query import FieldFilter
import firebase_config # Importar para asegurar que se inicialice
from firebase_helpers import db, upload_to_storage, log_analysis_in_firestore, extraer_metadatos_df, log_file_upload_in_firestore, descargar_contenido_de_storage, log_report_generation
from pydantic import BaseModel, Field, EmailStr
from io import StringIO
import openpyxl
from typing import Optional, Dict, Any, List, Literal, Callable # Any para pd.ExcelWriter
from datetime import datetime, timedelta, time, timezone # Para pd.Timestamp.now()
from track_expenses import process_csv, summarise_expenses, clean_data, get_top_expenses_by_month
from track_expenses import process_csv_abc, procesar_stock_muerto
from track_expenses import process_csv_puntos_alerta_stock
from track_expenses import process_csv_lista_basica_reposicion_historico, process_csv_analisis_estrategico_rotacion
from track_expenses import generar_reporte_maestro_inventario, auditar_margenes_de_productos_nuevo
from track_expenses import auditar_margenes_de_productos, diagnosticar_catalogo, auditar_calidad_datos
from track_expenses import generar_auditoria_inventario
from report_config import REPORTS_CONFIG
from plan_config import PLANS_CONFIG
from strategy_config import DEFAULT_STRATEGY
from tooltips_config import TOOLTIPS_GLOSSARY, KPI_TOOLTIPS_GLOSSARY

INITIAL_CREDITS = 25



# Leemos la variable de entorno. Si no existe, asumimos que estamos en 'development'.
ENVIRONMENT = os.environ.get("ENVIRONMENT", "development")

# Preparamos los argumentos para FastAPI
fastapi_kwargs = {
    "title": "Ferretero.IA API",
    "description": "API para anÃ¡lisis de datos de ferreterÃ­as.",
    "version": "1.0.0"
}

# Si estamos en producciÃ³n, desactivamos la documentaciÃ³n
if ENVIRONMENT == "production":
    print("ðŸš€ Iniciando en modo PRODUCCIÃ“N: La documentaciÃ³n de la API estÃ¡ desactivada.")
    fastapi_kwargs["docs_url"] = None
    fastapi_kwargs["redoc_url"] = None
    fastapi_kwargs["openapi_url"] = None
else:
    print("ðŸ”§ Iniciando en modo DESARROLLO: La documentaciÃ³n de la API estÃ¡ activa en /docs.")

# Inicializamos la aplicaciÃ³n con los argumentos correctos
app = FastAPI(**fastapi_kwargs)

# allow frontend to connect to backend
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "https://rentabilizate.ferreteros.app",
        "https://inteligencia.ferreteros.app",
        "https://analisis.ferreteros.app",
        "https://ia.ferreteros.app"
        # "*"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "Backend is running"}

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000)) # Default to 8000 if not set
    uvicorn.run(app, host="0.0.0.0", port=port)

# En tu main.py
@app.get("/healthcheck", status_code=200, tags=["Health"])
async def health_check():
    return {"status": "ok"}

@app.get("/reports-config", summary="Obtiene la configuraciÃ³n de los reportes disponibles", tags=["ConfiguraciÃ³n"])
async def get_reports_configuration():
    """
    Devuelve la lista de reportes disponibles con sus propiedades (costo, si es Pro, etc.).
    El frontend usarÃ¡ esto para construir dinÃ¡micamente la interfaz.
    """
    return JSONResponse(content={
            "reports": REPORTS_CONFIG,
            "tooltips": TOOLTIPS_GLOSSARY,
            "kpi_tooltips": KPI_TOOLTIPS_GLOSSARY
        })


# ===================================================================================
# --- CONFIGURACIÃ“N DE SEGURIDAD ---
# ===================================================================================

# Clave secreta para firmar los tokens. En producciÃ³n, Â¡debe estar en una variable de entorno!
SECRET_KEY = os.getenv("SECRET_KEY", "una-clave-secreta-muy-segura-para-desarrollo")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7 # Token vÃ¡lido por 7 dÃ­as

# ConfiguraciÃ³n para el hasheo de contraseÃ±as
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token", auto_error=False)

# ===================================================================================
# --- MODELOS DE DATOS (PYDANTIC) PARA USUARIOS Y TOKENS ---
# ===================================================================================
class User(BaseModel):
    email: str
    rol: str

class UserInDB(User):
    hashed_password: str

class Token(BaseModel):
    access_token: str
    token_type: str

class TokenData(BaseModel):
    email: Optional[str] = None


# ===================================================================================
# --- FUNCIONES AUXILIARES DE SEGURIDAD ---
# ===================================================================================

def verify_password(plain_password, hashed_password):
    """Verifica si una contraseÃ±a en texto plano coincide con su hash."""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    """Genera el hash de una contraseÃ±a."""
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """Crea un nuevo token de acceso (JWT)."""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        expire = datetime.now(timezone.utc) + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

   
# ===================================================================================
# --- NUEVOS ENDPOINTS PARA GESTIONAR LOS WORKSPACES ---
# ===================================================================================
async def get_current_user(token: str = Depends(oauth2_scheme)):
    """
    Decodifica el token JWT para obtener la informaciÃ³n del usuario.
    Esta funciÃ³n se usarÃ¡ en todos los endpoints protegidos.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="No se pudieron validar las credenciales",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            raise credentials_exception
        token_data = TokenData(email=email)
    except JWTError:
        raise credentials_exception
    
    user_ref = db.collection('usuarios').document(token_data.email)
    user_doc = user_ref.get()
    if not user_doc.exists:
        raise credentials_exception
        
    return user_doc.to_dict()

async def get_current_user_optional(token: Optional[str] = Depends(oauth2_scheme)) -> Optional[dict]:
    """
    Intenta obtener el usuario autenticado desde el token.
    Si no hay token, devuelve None sin causar un error.
    """
    # Si el frontend no enviÃ³ una cabecera de autorizaciÃ³n, el token serÃ¡ None
    if token is None:
        return None

    # Si hay un token, intentamos validarlo como antes
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Token invÃ¡lido o expirado",
    )
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            return None # El token es vÃ¡lido pero no contiene la informaciÃ³n esperada
        
        user_ref = db.collection('usuarios').document(email)
        user_doc = user_ref.get()
        if not user_doc.exists:
            return None # El usuario en el token ya no existe en la base de datos
            
        return user_doc.to_dict()
    except JWTError:
        # El token es invÃ¡lido o ha expirado
        return None

def get_metadata_from_context(base_ref):
    """
    Lee los metadatos cacheados directamente desde los documentos de Firestore
    para una carga de estado ultra-rÃ¡pida.
    """
    state = {
        "files": {"ventas": None, "inventario": None},
        "files_metadata": {"ventas": None, "inventario": None}
    }
    files_ref = base_ref.collection('archivos_cargados')

    # Buscamos el Ãºltimo archivo de ventas y leemos su metadata
    query_ventas = files_ref.where(filter=FieldFilter("tipoArchivo", "==", "ventas")).order_by("fechaCarga", direction="DESCENDING").limit(1).stream()
    last_venta_doc = next(query_ventas, None)
    if last_venta_doc:
        state["files"]["ventas"] = last_venta_doc.id
        ventas_metadata = last_venta_doc.to_dict().get("metadata", {})
        # --- CAMBIO CLAVE: Convertimos las fechas en los metadatos ANTES de guardarlos ---
        if 'fecha_primera_venta' in ventas_metadata and hasattr(ventas_metadata['fecha_primera_venta'], 'isoformat'):
            ventas_metadata['fecha_primera_venta'] = ventas_metadata['fecha_primera_venta'].isoformat()
        if 'fecha_ultima_venta' in ventas_metadata and hasattr(ventas_metadata['fecha_ultima_venta'], 'isoformat'):
            ventas_metadata['fecha_ultima_venta'] = ventas_metadata['fecha_ultima_venta'].isoformat()

        state["files_metadata"]["ventas"] = ventas_metadata
            
    # Buscamos el Ãºltimo archivo de inventario y leemos su metadata
    query_inventario = files_ref.where(filter=FieldFilter("tipoArchivo", "==", "inventario")).order_by("fechaCarga", direction="DESCENDING").limit(1).stream()
    last_inventario_doc = next(query_inventario, None)
    if last_inventario_doc:
        state["files"]["inventario"] = last_inventario_doc.id
        state["files_metadata"]["inventario"] = last_inventario_doc.to_dict().get("metadata", {})
        
    return state


# ===================================================================================
# --- FUNCIÃ“N DE AUDITORÃA ---
# ===================================================================================


def clean_for_json(obj: Any) -> Any:
    """
    Recorre recursivamente un objeto (diccionarios, listas) y reemplaza
    los valores no compatibles con JSON (NaN, inf) por None.
    """
    if isinstance(obj, dict):
        return {k: clean_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_for_json(elem) for elem in obj]
    # Usamos pd.isna() porque es mÃ¡s robusto y maneja NaN de numpy, float y pandas
    elif pd.isna(obj):
        return None
    elif isinstance(obj, float) and np.isinf(obj):
        return None
    return obj


def _parse_kpi_value(kpi_value: Any) -> float:
    """
    Extrae el valor numÃ©rico de un string de KPI, sin importar el formato.
    Puede manejar: 'S/ 1,234.50', '5.1 veces', '95.5%', '-10.2', etc.
    """
    # Si ya es un nÃºmero, lo devolvemos directamente.
    if isinstance(kpi_value, (int, float)):
        return float(kpi_value)
        
    # Si no es un string, no podemos procesarlo.
    if not isinstance(kpi_value, str):
        return 0.0

    try:
        # --- LÃ“GICA DE "TRADUCCIÃ“N" MEJORADA ---
        # 1. Limpiamos el string de elementos comunes como el sÃ­mbolo de Soles y las comas.
        cleaned_string = kpi_value.replace("S/ ", "").replace(",", "").strip()
        
        # 2. Usamos una expresiÃ³n regular para encontrar el primer nÃºmero (entero o decimal)
        #    al principio del string. Esto encontrarÃ¡ "5.1" en "5.1 veces" o "95.5" en "95.5%".
        match = re.match(r'^-?(\d*\.?\d+)', cleaned_string)
        
        if match:
            # Si encontramos un nÃºmero, lo convertimos a float y lo devolvemos.
            return float(match.group(1))
        else:
            # Si no se encuentra ningÃºn nÃºmero, devolvemos 0 como un valor seguro.
            return 0.0
            
    except (ValueError, TypeError):
        # Si ocurre cualquier error durante la conversiÃ³n, devolvemos 0.
        return 0.0


def comparar_auditorias(actual: Dict[str, Any], previa: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Compara dos resultados de auditorÃ­a y genera el 'Informe de EvoluciÃ³n'.
    """
    if not previa:
        # Si no hay auditorÃ­a previa, devolvemos un informe base sin comparaciÃ³n.
        return {
            "tipo": "inicial",
            "fecha_actual": actual.get("fecha"),
            "puntaje_salud": actual.get("puntaje_salud"),
            "kpis_dolor": actual.get("kpis_dolor", {}),
            "plan_de_accion": actual.get("plan_de_accion", []),
            "source_files": actual.get("source_files", {})
        }

    kpi_success_direction = {
        "Capital en Riesgo (S/.)": False,
        "Venta Perdida Potencial (S/.)": False,
        "Eficiencia de Margen (%)": True,
        "RotaciÃ³n Anual Estimada": True,
        # AÃ±ade aquÃ­ otros KPIs que puedan aparecer en el futuro
    }

    # --- Componente 1: El "Antes y DespuÃ©s" Cuantificado ---
    kpis_con_delta = {}
    kpis_actuales = actual.get("kpis_dolor", {})
    kpis_previos = previa.get("kpis_dolor", {})

    for key, current_value_str in kpis_actuales.items():
        previous_value_str = kpis_previos.get(key, "0")
        current_value = _parse_kpi_value(current_value_str)
        previous_value = _parse_kpi_value(previous_value_str)
        delta = current_value - previous_value
        # kpis_con_delta[key] = {"actual": current_value_str, "delta": f"{delta:+.2f}"}
        delta_type = "neutral" # Por defecto
        if delta != 0:
            # Obtenemos la regla para este KPI, con un default de True
            increase_is_good = kpi_success_direction.get(key, True)
            if (delta > 0 and increase_is_good) or (delta < 0 and not increase_is_good):
                delta_type = "positivo"
            else:
                delta_type = "negativo"
        
        kpis_con_delta[key] = {
            "actual": current_value_str,
            "delta": f"{delta:+.2f}",
            "delta_type": delta_type # <-- AÃ±adimos la nueva clave
        }
        

    puntaje_delta = actual.get("puntaje_salud", 0) - previa.get("puntaje_salud", 0)

    # --- Componente 2: El "Log de Eventos de Negocio" ---
    tareas_actuales_ids = {task['id'] for task in actual.get("plan_de_accion", [])}
    tareas_previas_ids = {task['id'] for task in previa.get("plan_de_accion", [])}
    nuevos_problemas_ids = tareas_actuales_ids - tareas_previas_ids
    problemas_resueltos_ids = tareas_previas_ids - tareas_actuales_ids
    nuevos_problemas = [task for task in actual.get("plan_de_accion", []) if task['id'] in nuevos_problemas_ids]
    problemas_resueltos = [task for task in previa.get("plan_de_accion", []) if task['id'] in problemas_resueltos_ids]

    return {
        "tipo": "evolucion",
        "fecha_actual": actual.get("fecha"),
        "fecha_previa": previa.get("fecha"),
        "puntaje_actual": actual.get("puntaje_salud"),
        "puntaje_delta": f"{puntaje_delta:+}",
        "kpis_con_delta": kpis_con_delta,
        "log_eventos": {
            "nuevos_problemas": nuevos_problemas,
            "problemas_resueltos": problemas_resueltos
        },
        "plan_de_accion": actual.get("plan_de_accion", []),
        "source_files": actual.get("source_files", {})
    }


@app.post("/auditoria-inicial", summary="Ejecuta la auditorÃ­a de eficiencia inicial", tags=["AuditorÃ­a"])
async def ejecutar_auditoria_inicial(
    request: Request,
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: Optional[str] = Header(None, alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),
    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...)
):
    """
    Este endpoint se dedica a ejecutar la auditorÃ­a inicial.
    Llama a la funciÃ³n de lÃ³gica directamente porque su formato de respuesta es diferente.
    """
    # 1. Determinamos el contexto
    user_id = current_user['email'] if current_user else None
    if user_id and not workspace_id:
        raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
    
    try:
        # 2. Cargamos los DataFrames
        ventas_contents = await descargar_contenido_de_storage(user_id, workspace_id, X_Session_ID, ventas_file_id)
        inventario_contents = await descargar_contenido_de_storage(user_id, workspace_id, X_Session_ID, inventario_file_id)
        df_ventas = pd.read_csv(io.BytesIO(ventas_contents))
        df_inventario = pd.read_csv(io.BytesIO(inventario_contents))

        # 3. Llamamos a la funciÃ³n de lÃ³gica que devuelve el resumen
        auditoria_actual = generar_auditoria_inventario(df_ventas, df_inventario)
        now_iso = datetime.now(timezone.utc).isoformat()
        auditoria_actual["fecha"] = now_iso

        # --- FASE 2: BÃºsqueda de la AuditorÃ­a Previa ---
        if user_id and workspace_id:
            base_ref = db.collection('usuarios').document(user_id).collection('espacios_trabajo').document(workspace_id)
        elif X_Session_ID:
            base_ref = db.collection('sesiones_anonimas').document(X_Session_ID)
        
        auditorias_ref = base_ref.collection('auditorias_historicas')
        query = auditorias_ref.order_by("fecha", direction=firestore.Query.DESCENDING).limit(1)
        
        auditoria_previa_doc = next(query.stream(), None)
        auditoria_previa = auditoria_previa_doc.to_dict() if auditoria_previa_doc else None

        # 3. Comparamos y generamos el informe de evoluciÃ³n
        informe_evolucion_raw = comparar_auditorias(auditoria_actual, auditoria_previa)

        # --- CAMBIO CLAVE: Aplicamos el "Control de Calidad" Final ---
        print("Ejecutando limpieza profunda en el Informe de EvoluciÃ³n...")
        informe_evolucion_clean = clean_for_json(informe_evolucion_raw)
        print("âœ… Limpieza completada.")
        
        # 4. Guardamos la auditorÃ­a actual (la versiÃ³n "cruda", antes de la limpieza)
        auditorias_ref.document(now_iso.replace(":", "-")).set(auditoria_actual)
        
        # 5. Devolvemos el resultado ya limpio
        return JSONResponse(content=informe_evolucion_clean)

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"OcurriÃ³ un error crÃ­tico durante la auditorÃ­a: {e}")




# --- ENDPOINT 1: El "Comparador de Versiones" (RÃ¡pido) ---
@app.get("/auditoria/status", summary="Verifica si la Ãºltima auditorÃ­a estÃ¡ actualizada", tags=["AuditorÃ­a"])
async def get_audit_status(
    # La firma de la funciÃ³n recibe el contexto del usuario
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: Optional[str] = Header(None, alias="X-Session-ID"),
    workspace_id: Optional[str] = Query(None)
):
    """
    Compara los archivos actuales con los usados en la Ãºltima auditorÃ­a guardada
    y devuelve el estado y el INFORME COMPLETO si estÃ¡ actualizado.
    """
    try:
        # 1. Identificar el contexto y la referencia base
        user_id = current_user['email'] if current_user else None
        if user_id and not workspace_id:
            raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        
        if user_id:
            base_ref = db.collection('usuarios').document(user_id).collection('espacios_trabajo').document(workspace_id)
        elif X_Session_ID:
            base_ref = db.collection('sesiones_anonimas').document(X_Session_ID)
        else:
            raise HTTPException(status_code=401, detail="No se proporcionÃ³ contexto de autenticaciÃ³n.")

        # 2. Buscar los IDs de los Ãºltimos archivos subidos
        files_ref = base_ref.collection('archivos_cargados')
        last_venta_doc = next(files_ref.where("tipoArchivo", "==", "ventas").order_by("fechaCarga", direction="DESCENDING").limit(1).stream(), None)
        last_inventario_doc = next(files_ref.where("tipoArchivo", "==", "inventario").order_by("fechaCarga", direction="DESCENDING").limit(1).stream(), None)
        
        current_venta_id = last_venta_doc.id if last_venta_doc else None
        current_inventario_id = last_inventario_doc.id if last_inventario_doc else None

        # 3. Buscar el Ãºnico documento de auditorÃ­a guardado
        audit_ref = base_ref.collection('auditorias').document('latest')
        last_audit_doc = audit_ref.get()

        if not last_audit_doc.exists:
            return JSONResponse(content={"status": "no_audit_found", "data": None})

        last_audit_data_raw = last_audit_doc.to_dict()
        last_audit_data_clean = clean_for_json(last_audit_data_raw)
        source_files = last_audit_data_clean.get("source_files", {})

        # 4. La ComparaciÃ³n Inteligente
        if source_files.get("ventas_id") == current_venta_id and source_files.get("inventario_id") == current_inventario_id:
            # Cache Hit: Devolvemos el informe completo que ya estaba guardado
            return JSONResponse(content={"status": "up_to_date", "data": last_audit_data_clean})
        else:
            # Cache Miss: Devolvemos el informe antiguo para que el usuario pueda verlo
            return JSONResponse(content={"status": "outdated", "data": last_audit_data_clean})


    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error al verificar estado de auditorÃ­a: {e}")



# --- ENDPOINT 2: La EjecuciÃ³n "Bajo Demanda" (Pesado) ---
@app.post("/auditoria/run", summary="Ejecuta, compara y guarda un nuevo Informe de EvoluciÃ³n", tags=["AuditorÃ­a"])
async def run_new_audit(
    request: Request,
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: Optional[str] = Header(None, alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),
    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...)
):
    """
    Implementa el "Registro Ãšnico y Dorado". Ejecuta una nueva auditorÃ­a,
    la compara con la anterior, guarda el informe de evoluciÃ³n completo y lo devuelve.
    """
    try:
        # 1. Determinar el contexto
        user_id = current_user['email'] if current_user else None
        if user_id and not workspace_id:
            raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        
        if user_id:
            base_ref = db.collection('usuarios').document(user_id).collection('espacios_trabajo').document(workspace_id)
        elif X_Session_ID:
            base_ref = db.collection('sesiones_anonimas').document(X_Session_ID)
        else:
            raise HTTPException(status_code=400, detail="Contexto no vÃ¡lido.")
        
        audit_ref = base_ref.collection('auditorias').document('latest')

        # --- FASE 1: RotaciÃ³n (Leer 'latest' y guardarlo como 'previa') ---
        print("Fase 1: Rotando la auditorÃ­a anterior...")
        # Leemos el informe anterior directamente desde nuestro "registro dorado"
        informe_previo = audit_ref.get().to_dict() if audit_ref.get().exists else None

        auditoria_previa = None
        if informe_previo:
            print("Informe previo encontrado. Extrayendo 'snapshot' para comparaciÃ³n...")
            # Extraemos la "foto" de la auditorÃ­a anterior del informe previo
            kpis_previos_con_delta = informe_previo.get("kpis_con_delta", {})
            kpis_previos_simples = {key: data.get("actual") for key, data in kpis_previos_con_delta.items()} if informe_previo.get("tipo") == "evolucion" else informe_previo.get("kpis_dolor")
            
            auditoria_previa = {
                "fecha": informe_previo.get("fecha_actual") or informe_previo.get("fecha"),
                "puntaje_salud": informe_previo.get("puntaje_actual") or informe_previo.get("puntaje_salud"),
                "kpis_dolor": kpis_previos_simples,
                "plan_de_accion": informe_previo.get("plan_de_accion", [])
            }

        # --- FASE 2: EjecuciÃ³n ---
        print("Fase 2: Ejecutando la nueva auditorÃ­a...")
        # ... (tu lÃ³gica para cargar df_ventas y df_inventario)
        ventas_contents = await descargar_contenido_de_storage(user_id, workspace_id, X_Session_ID, ventas_file_id)
        inventario_contents = await descargar_contenido_de_storage(user_id, workspace_id, X_Session_ID, inventario_file_id)
        df_ventas = pd.read_csv(io.BytesIO(ventas_contents))
        df_inventario = pd.read_csv(io.BytesIO(inventario_contents))

        auditoria_actual = generar_auditoria_inventario(df_ventas, df_inventario)
        now_iso = datetime.now(timezone.utc).isoformat()
        auditoria_actual["fecha"] = now_iso
        auditoria_actual["source_files"] = { "ventas_id": ventas_file_id, "inventario_id": inventario_file_id }
        
        print(f"Incrementando contador de auditorÃ­as para el workspace: {workspace_id}")
        base_ref.update({"auditorias_ejecutadas": firestore.Increment(1)})

        # --- FASE 3: ComparaciÃ³n y GeneraciÃ³n del Informe de EvoluciÃ³n ---
        informe_evolucion_raw = comparar_auditorias(auditoria_actual, auditoria_previa)
        

        # --- FASE 4: Guardado Persistente ---
        # Guardamos el informe de evoluciÃ³n completo en nuestro "registro dorado", sobrescribiendo el anterior
        audit_ref.set(informe_evolucion_raw)

        # --- FASE 5: Respuesta ---
        informe_evolucion_clean = clean_for_json(informe_evolucion_raw)
        return JSONResponse(content=informe_evolucion_clean)

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"OcurriÃ³ un error crÃ­tico al ejecutar la auditorÃ­a: {e}")


# ===================================================================================
# --- MODELOS DE DATOS ---
# ===================================================================================

# --- NUEVO MODELO DE DATOS PARA ONBOARDING ---
class OnboardingData(BaseModel):
    """Define la estructura de datos que esperamos recibir del formulario de onboarding."""
    rol: str = Field(..., description="El rol seleccionado por el usuario (e.g., 'dueÃ±o', 'consultor').")

# --- MODELO DE DATOS PARA VALIDAR LA ESTRATEGIA ---
class StrategyData(BaseModel):
    score_ventas: int = Field(..., ge=1, le=10)
    score_ingreso: int = Field(..., ge=1, le=10)
    score_margen: int = Field(..., ge=1, le=10)
    score_dias_venta: int = Field(..., ge=1, le=10)
    lead_time_dias: int = Field(..., ge=0)
    dias_cobertura_ideal_base: int = Field(..., ge=1)
    dias_seguridad_base: int = Field(..., ge=0)
    dias_analisis_ventas_recientes: int = Field(..., ge=1)
    dias_analisis_ventas_general: int = Field(..., ge=1)
    excluir_sin_ventas: str
    peso_ventas_historicas: float = Field(..., ge=0, le=1)


# ===================================================================================
# --- AUDITORIA DE CREDITOS ---
# ===================================================================================
@app.post("/admin/recharge", summary="[ADMIN] Recarga crÃ©ditos a un usuario", tags=["AdministraciÃ³n"])
async def admin_recharge_credits(
    secret_key: str = Form(...),
    user_email: str = Form(...),
    credits_to_add: int = Form(...),
    reason: Optional[str] = Form("Recarga manual de administrador")
):
    """
    Endpoint privado para que el administrador aÃ±ada crÃ©ditos a una cuenta de usuario.
    Requiere una clave secreta para la autorizaciÃ³n.
    """
    # 1. VerificaciÃ³n de la "Llave Maestra"
    # La clave se lee de una variable de entorno para mÃ¡xima seguridad.
    ADMIN_KEY = os.environ.get("ADMIN_SECRET_KEY")
    if not ADMIN_KEY or secret_key != ADMIN_KEY:
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Acceso denegado: Clave secreta invÃ¡lida.")

    # 2. LÃ³gica de Recarga
    try:
        user_ref = db.collection('usuarios').document(user_email)
        user_doc = user_ref.get()

        if not user_doc.exists:
            raise HTTPException(status_code=404, detail=f"Usuario '{user_email}' no encontrado.")

        # Usamos una transacciÃ³n para seguridad y consistencia
        @firestore.transactional
        def recharge_transaction(transaction, user_ref):
            # Incrementamos los crÃ©ditos del usuario
            transaction.update(user_ref, {
                "creditos_iniciales": firestore.Increment(credits_to_add),
                "creditos_restantes": firestore.Increment(credits_to_add)
            })
            
            # Creamos un registro de auditorÃ­a
            audit_ref = user_ref.collection('auditoria_creditos').document()
            transaction.set(audit_ref, {
                "fecha": datetime.now(timezone.utc),
                "cantidad": credits_to_add,
                "motivo": reason,
                "tipo": "recarga_admin"
            })

        transaction = db.transaction()
        recharge_transaction(transaction, user_ref)
        
        print(f"âœ… RECARGA ADMIN: Se aÃ±adieron {credits_to_add} crÃ©ditos a {user_email}.")
        return {"message": f"Ã‰xito: Se han aÃ±adido {credits_to_add} crÃ©ditos a la cuenta de {user_email}."}

    except Exception as e:
        print(f"ðŸ”¥ ERROR ADMIN RECHARGE: {e}")
        raise HTTPException(status_code=500, detail="OcurriÃ³ un error al procesar la recarga.")



# ===================================================================================
# --- ENDPOINTS DE AUTENTICACIÃ“N ---
# ===================================================================================
@app.post("/register", summary="Registra un nuevo usuario y crea su primer espacio de trabajo", tags=["Usuarios"])
async def register_user(
    request: Request,
    email: str = Form(...),
    password: str = Form(...),
    rol: str = Form(...),
    # La sesiÃ³n anÃ³nima sigue siendo opcional, para una futura migraciÃ³n de datos
    X_Session_ID: Optional[str] = Header(None, alias="X-Session-ID")
):
    """
    Crea una nueva cuenta de usuario y, crucialmente, inicializa su primer
    espacio de trabajo con una estrategia por defecto.
    """
    user_ref = db.collection('usuarios').document(email)
    if user_ref.get().exists:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Ya existe un usuario con este correo electrÃ³nico.",
        )

    # --- LÃ³gica de GeolocalizaciÃ³n (idÃ©ntica a la de sesiones anÃ³nimas) ---
    client_ip = request.client.host
    geoloc_data = {"ip": client_ip, "status": "desconocido"}
    if client_ip and client_ip not in ["127.0.0.1", "testclient"]:
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"http://ip-api.com/json/{client_ip}")
                response.raise_for_status()
                api_data = response.json()
                if api_data.get("status") == "success":
                    geoloc_data = {
                        "ip": api_data.get("query"), "pais": api_data.get("country"),
                        "ciudad": api_data.get("city"), "region": api_data.get("regionName"),
                        "isp": api_data.get("isp"), "status": "exitoso"
                    }
        except httpx.RequestError as e:
            print(f"ðŸ”¥ Advertencia: No se pudo geolocalizar al nuevo usuario {email}. Error: {e}")
      
        
    hashed_password = get_password_hash(password)
    now_utc = datetime.now(timezone.utc)

    # Datos principales del usuario
    new_user_data = {
        "email": email,
        "rol": rol,
        "hashed_password": hashed_password,
        "fechaRegistro": now_utc,
        "creditos_iniciales": 25, # Bono de bienvenida para usuarios registrados
        "creditos_restantes": 25, # Bono de bienvenida para usuarios registrados
        "plan": "gratis",
        "estrategia_global": DEFAULT_STRATEGY,
        "geolocalizacion_registro": geoloc_data
    }
    
    # --- LÃ“GICA CLAVE: CREACIÃ“N DEL PRIMER ESPACIO DE TRABAJO ---
    # Creamos la referencia al nuevo documento del usuario
    batch = db.batch()
    batch.set(user_ref, new_user_data)

    # Creamos la referencia a la sub-colecciÃ³n de espacios de trabajo
    workspaces_ref = user_ref.collection('espacios_trabajo')
    
    # Creamos el primer espacio de trabajo por defecto
    default_workspace_data = {
        "nombre": "Mi Primera FerreterÃ­a",
        "fechaCreacion": now_utc,
        "fechaUltimoAcceso": now_utc,
        "fechaUltimoAcceso": now_utc,
        "isPinned": False # Inicializamos el campo de fijado
    }
    # Creamos un nuevo documento para el espacio de trabajo
    # (podrÃ­amos usar un ID autogenerado o uno predecible)
    new_workspace_ref = workspaces_ref.document("default_workspace")
    batch.set(new_workspace_ref, default_workspace_data)
    
    # AquÃ­ irÃ­a la lÃ³gica para migrar los archivos de la sesiÃ³n anÃ³nima (si existe X_Session_ID)
    # a la sub-colecciÃ³n 'archivos_cargados' de este nuevo espacio de trabajo.
    
    # Ejecutamos todas las operaciones en una sola transacciÃ³n
    batch.commit()

    return {"message": "Usuario registrado exitosamente. Tu primer espacio de trabajo 'Mi Primera FerreterÃ­a' ha sido creado."}


@app.post("/token", response_model=Token, summary="Inicia sesiÃ³n y obtiene un token", tags=["Usuarios"])
async def login_for_access_token(
    request: Request,
    form_data: OAuth2PasswordRequestForm = Depends()
):
    """
    Autentica a un usuario y devuelve un token JWT para usar en las siguientes peticiones.
    FastAPI usa el campo 'username' para el email.
    """
    email = form_data.username
    password = form_data.password

    user_ref = db.collection('usuarios').document(email)
    user_doc = user_ref.get()

    if not user_doc.exists:
        raise HTTPException(status_code=400, detail="Correo o contraseÃ±a incorrectos")

    # --- LÃ³gica de GeolocalizaciÃ³n al iniciar sesiÃ³n ---
    client_ip = request.client.host
    if client_ip and client_ip not in ["127.0.0.1", "testclient"]:
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"http://ip-api.com/json/{client_ip}")
                if response.status_code == 200:
                    api_data = response.json()
                    if api_data.get("status") == "success":
                        geoloc_data = {
                            "ip": api_data.get("query"), "pais": api_data.get("country"),
                            "ciudad": api_data.get("city"), "region": api_data.get("regionName"),
                            "timestamp": datetime.now(timezone.utc) # <-- AÃ±adimos un timestamp
                        }
                        # Actualizamos el documento del usuario con la nueva ubicaciÃ³n
                        user_ref.update({"geolocalizacion_ultimo_login": geoloc_data})
                        print(f"âœ… UbicaciÃ³n de login actualizada para {email}.")
        except httpx.RequestError as e:
            print(f"ðŸ”¥ Advertencia: No se pudo geolocalizar el login del usuario {email}. Error: {e}")

    user_data = user_doc.to_dict()
    if not verify_password(password, user_data.get("hashed_password")):
        raise HTTPException(status_code=400, detail="Correo o contraseÃ±a incorrectos")

    # Si la autenticaciÃ³n es exitosa, creamos el token
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user_data["email"]}, expires_delta=access_token_expires
    )

    return {"access_token": access_token, "token_type": "bearer"}



# ===================================================================================
# --- NUEVO ENDPOINT PARA CREAR SESIONES ---
# ===================================================================================
@app.post("/sessions", summary="Crea una sesiÃ³n y devuelve su estado inicial", tags=["SesiÃ³n"])
async def create_analysis_session(
    request: Request, # Inyectamos el objeto Request para obtener la IP
    onboarding_data: OnboardingData
):
    """
    Inicia una nueva sesiÃ³n, la guarda en Firestore, y devuelve el ID
    junto con la estrategia por defecto para evitar una segunda llamada a la API.
    """
    try:
        session_id = str(uuid.uuid4())
        now = datetime.now(timezone.utc)


        # --- INICIO DE LA LÃ“GICA DE GEOLOCALIZACIÃ“N ---
    
        # Obtenemos la IP del cliente desde el objeto Request
        client_ip = request.client.host
        geoloc_data = {"ip": client_ip, "status": "desconocido"}

        # Evitamos llamar a la API para IPs locales o de prueba
        if client_ip and client_ip not in ["127.0.0.1", "testclient"]:
            try:
                # Usamos httpx para hacer una llamada asÃ­ncrona a la API de geolocalizaciÃ³n
                async with httpx.AsyncClient() as client:
                    response = await client.get(f"http://ip-api.com/json/{client_ip}")
                    response.raise_for_status() # Lanza un error si la respuesta no es 2xx
                    
                    # Guardamos los datos si la peticiÃ³n fue exitosa
                    api_data = response.json()
                    if api_data.get("status") == "success":
                        geoloc_data = {
                            "ip": api_data.get("query"),
                            "pais": api_data.get("country"),
                            "ciudad": api_data.get("city"),
                            "region": api_data.get("regionName"),
                            "isp": api_data.get("isp"),
                            "status": "exitoso"
                        }
                    else:
                        geoloc_data["status"] = "fallido_api"
                        geoloc_data["error_message"] = api_data.get("message")

            except httpx.RequestError as e:
                print(f"ðŸ”¥ Error de red al consultar la API de geolocalizaciÃ³n: {e}")
                geoloc_data["status"] = "fallido_red"
                geoloc_data["error_message"] = str(e)
        
        # --- FIN DE LA LÃ“GICA DE GEOLOCALIZACIÃ“N ---


        session_ref = db.collection('sesiones_anonimas').document(session_id)
        
        session_log = {
            "fechaCreacion": now,
            "onboardingData": {"rol": onboarding_data.rol},
            "ultimoAcceso": now,
            "creditos_iniciales": INITIAL_CREDITS,
            "creditos_restantes": INITIAL_CREDITS,
            "estrategia": DEFAULT_STRATEGY,
            "geolocalizacion": geoloc_data # <-- Guardamos el nuevo objeto en Firestore
        }
        
        session_ref.set(session_log)
        
        print(f"âœ… Nueva sesiÃ³n creada para IP {client_ip} desde {geoloc_data.get('ciudad', 'N/A')}. ID: {session_id}")
        
        # --- CAMBIO CLAVE: Devolvemos tanto el ID como la estrategia ---
        return JSONResponse(content={
            "sessionId": session_id,
            "strategy": DEFAULT_STRATEGY
        })

    except Exception as e:
        print(f"ðŸ”¥ Error al crear la sesiÃ³n en Firestore: {e}")
        raise HTTPException(status_code=500, detail="No se pudo crear la sesiÃ³n en el servidor.")

# ===================================================================================
# --- NUEVO ENDPOINT PARA RECUPERAR EL ESTADO DE LA SESIÃ“N ---
# ===================================================================================
@app.get("/session-state", summary="Recupera los crÃ©ditos y el historial para una sesiÃ³n", tags=["SesiÃ³n"])
async def get_session_state(
    X_Session_ID: str = Header(..., alias="X-Session-ID")
):
    """
    Obtiene el estado completo de una sesiÃ³n anÃ³nima, incluyendo crÃ©ditos,
    historial y metadatos de archivos, con un manejo de fechas robusto.
    """
    if not X_Session_ID:
        raise HTTPException(status_code=400, detail="La cabecera X-Session-ID es requerida.")

    try:
        session_ref = db.collection('sesiones_anonimas').document(X_Session_ID)
        session_doc = session_ref.get()

        if not session_doc.exists:
            raise HTTPException(status_code=404, detail="La sesiÃ³n no existe o ha expirado.")

        session_data = session_doc.to_dict()
        
        # --- 1. Obtener crÃ©ditos ---
        creditos_restantes = session_data.get("creditos_restantes", 0)
        creditos_usados = session_data.get("creditos_iniciales", 20) - creditos_restantes
        credits_data = {"used": creditos_usados, "remaining": creditos_restantes}

        # --- 2. Obtener el historial de reportes ---
        historial_ref = session_ref.collection('reportes_generados')
        query = historial_ref.order_by("fechaGeneracion", direction=firestore.Query.DESCENDING).limit(10)
        
        historial_list = []
        for doc in query.stream():
            doc_data = doc.to_dict()
            # Usamos hasattr para una conversiÃ³n de fecha segura
            if 'fechaGeneracion' in doc_data and hasattr(doc_data['fechaGeneracion'], 'isoformat'):
                doc_data['fechaGeneracion'] = doc_data['fechaGeneracion'].isoformat()
            historial_list.append(doc_data)

        # --- 3. Obtener metadatos de archivos (LÃ³gica Optimizada) ---
        files_ref = session_ref.collection('archivos_cargados')
        
        files_map = {"ventas": None, "inventario": None}
        date_range_bounds = None
        available_filters = {"categorias": [], "marcas": []}

        # Usamos la sintaxis posicional estable para las consultas
        query_ventas = files_ref.where(filter=FieldFilter("tipoArchivo", "==", "ventas")).order_by("fechaCarga", direction="DESCENDING").limit(1).stream()
        last_venta_doc = next(query_ventas, None)
        if last_venta_doc:
            files_map["ventas"] = last_venta_doc.id
            metadata = last_venta_doc.to_dict().get("metadata", {})
        
        query_inventario = files_ref.where(filter=FieldFilter("tipoArchivo", "==", "inventario")).order_by("fechaCarga", direction="DESCENDING").limit(1).stream()
        last_inventario_doc = next(query_inventario, None)
        if last_inventario_doc:
            files_map["inventario"] = last_inventario_doc.id
            metadata = last_inventario_doc.to_dict().get("metadata", {})
            available_filters = {"categorias": metadata.get("lista_completa_categorias", []), "marcas": metadata.get("lista_completa_marcas", [])}

        # --- 4. Construir y devolver la respuesta completa ---

        date_range_bounds = session_data.get("fechas_disponibles") # Asumiendo que se guarda aquÃ­

        # --- INICIO DEL BLOQUE DE AUDITORÃA DE SERIALIZACIÃ“N ---
        # print("\n--- DEBUG: AuditorÃ­a de SerializaciÃ³n JSON ---")
        metadata_payload = get_metadata_from_context(session_ref)
        final_content = {
            "credits": credits_data,
            "history": historial_list,
            "files": files_map,
            "available_filters": available_filters,
            "date_range_bounds": date_range_bounds,
            **metadata_payload
        }
        
        # for key, value in final_content.items():
        #     try:
        #         json.dumps(value, default=str) # Usamos `default=str` como un fallback seguro
        #         print(f"âœ… La secciÃ³n '{key}' se puede serializar correctamente.")
        #         print(f"{value}")
        #     except TypeError as e:
        #         print(f"ðŸ”¥ðŸ”¥ðŸ”¥ Â¡ERROR ENCONTRADO! La secciÃ³n '{key}' no se puede serializar. Causa: {e}")
        #         print(f"Datos problemÃ¡ticos en '{key}': {value}")
        # print("--------------------------------------------\n")
        # --- FIN DEL BLOQUE DE AUDITORÃA ---



        return JSONResponse(content=final_content)

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        print(f"ðŸ”¥ Error al recuperar estado de sesiÃ³n para {X_Session_ID}: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="No se pudo recuperar el estado de la sesiÃ³n desde el servidor.")



# ===================================================================================
# --- NUEVO ENDPOINT PARA OBTENER EL ESTADO DE UN WORKSPACE ESPECÃFICO ---
# ===================================================================================
@app.get("/workspaces/{workspace_id}/state", summary="Recupera el estado de un espacio de trabajo (con cachÃ© de filtros)", tags=["Espacios de Trabajo"])
async def get_workspace_state(
    workspace_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    Obtiene el estado completo de un workspace, con un bloque de depuraciÃ³n
    para auditar la serializaciÃ³n de cada componente de la respuesta.
    """
    try:
        user_email = current_user.get("email")
        
        # --- LÃ³gica de obtenciÃ³n de datos (sin cambios) ---
        user_ref = db.collection('usuarios').document(user_email)
        workspace_ref = user_ref.collection('espacios_trabajo').document(workspace_id)
        
        user_doc = user_ref.get()
        workspace_doc = workspace_ref.get()

        if not user_doc.exists or not workspace_doc.exists:
            raise HTTPException(status_code=404, detail="Usuario o espacio de trabajo no encontrado.")
        
        user_data = user_doc.to_dict()
        workspace_data = workspace_doc.to_dict()


        # --- NUEVO: LÃ“GICA DE RESETEO DE CRÃ‰DITOS DIARIO ---
        now = datetime.now(timezone.utc)
        today_midnight_utc = datetime.combine(now.date(), time.min, tzinfo=timezone.utc)

        last_reset_time = user_data.get("ultimo_reseteo_creditos")
        current_credits = user_data.get("creditos_restantes", 0)
        user_plan = user_data.get("plan", "gratis")

        # CondiciÃ³n: El usuario es 'gratis', sus crÃ©ditos son < 25, y
        # el Ãºltimo reseteo fue antes de la medianoche de hoy (o nunca ha ocurrido).
        if user_plan == "gratis" and current_credits < 25 and (last_reset_time is None or last_reset_time < today_midnight_utc):
            print(f"âœ… Reseteando crÃ©ditos para el usuario {user_email}. Saldo actual: {current_credits}.")
            
            # Actualizamos el documento del usuario en Firestore
            user_ref.update({
                "creditos_restantes": 25,
                "ultimo_reseteo_creditos": now
            })
            
            # Actualizamos el diccionario local para que la respuesta sea correcta
            user_data["creditos_restantes"] = 25
            user_data["ultimo_reseteo_creditos"] = now
        # --- FIN DE LA LÃ“GICA DE RESETEO ---

        creditos_restantes = user_data.get("creditos_restantes", 0)
        creditos_usados = user_data.get("creditos_iniciales", 0) - creditos_restantes
        credits_data = {"used": creditos_usados, "remaining": creditos_restantes}

        historial_ref = workspace_ref.collection('reportes_generados')
        query_historial = historial_ref.order_by("fechaGeneracion", direction=firestore.Query.DESCENDING).limit(10)
        historial_list = []
        for doc in query_historial.stream():
            doc_data = doc.to_dict()
            for field in ['fechaGeneracion', 'fechaUltimoAcceso', 'fechaCreacion']:
                if field in doc_data and hasattr(doc_data[field], 'isoformat'):
                    doc_data[field] = doc_data[field].isoformat()
            historial_list.append(doc_data)

        files_ref = workspace_ref.collection('archivos_cargados')
        # --- CORRECCIÃ“N PROACTIVA: Volvemos a la sintaxis que sabemos que es estable ---
        query_ventas = files_ref.where(filter=FieldFilter("tipoArchivo", "==", "ventas")).order_by("fechaCarga", direction=firestore.Query.DESCENDING).limit(1).stream()
        query_inventario = files_ref.where(filter=FieldFilter("tipoArchivo", "==", "inventario")).order_by("fechaCarga", direction=firestore.Query.DESCENDING).limit(1).stream()
        
        last_venta_doc = next(query_ventas, None)
        last_inventario_doc = next(query_inventario, None)
        files_map = {"ventas": last_venta_doc.id if last_venta_doc else None, "inventario": last_inventario_doc.id if last_inventario_doc else None}
        
        available_filters = workspace_data.get("filtros_disponibles", {"categorias": [], "marcas": []})
        date_range_bounds = workspace_data.get("fechas_disponibles") # Asumiendo que se guarda aquÃ­

        # --- INICIO DEL BLOQUE DE AUDITORÃA DE SERIALIZACIÃ“N ---
        # print("\n--- DEBUG: AuditorÃ­a de SerializaciÃ³n JSON ---")
        metadata_payload = get_metadata_from_context(workspace_ref)

        auditorias_ejecutadas = workspace_data.get("auditorias_ejecutadas", 0)

        final_content = {
            "credits": credits_data,
            "history": historial_list,
            "files": files_map,
            "available_filters": available_filters,
            "date_range_bounds": date_range_bounds,
            "auditorias_ejecutadas": auditorias_ejecutadas,
            **metadata_payload
        }
        
        # for key, value in final_content.items():
        #     try:
        #         json.dumps(value, default=str) # Usamos `default=str` como un fallback seguro
        #         print(f"âœ… La secciÃ³n '{key}' se puede serializar correctamente.")
        #         print(f"{value}")
        #     except TypeError as e:
        #         print(f"ðŸ”¥ðŸ”¥ðŸ”¥ Â¡ERROR ENCONTRADO! La secciÃ³n '{key}' no se puede serializar. Causa: {e}")
        #         print(f"Datos problemÃ¡ticos en '{key}': {value}")
        # print("--------------------------------------------\n")
        # # --- FIN DEL BLOQUE DE AUDITORÃA ---

        return JSONResponse(content=final_content)

    except Exception as e:
        print(f"ðŸ”¥ Error al recuperar estado del workspace {workspace_id}: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="No se pudo recuperar el estado del espacio de trabajo.")


# ===================================================================================
# --- MODELOS DE DATOS (PYDANTIC) PARA LA CREACIÃ“N DE WORKSPACES ---
# ===================================================================================
class WorkspaceCreate(BaseModel):
    nombre: str = Field(..., min_length=3, max_length=50, description="El nombre del nuevo espacio de trabajo.")

class WorkspaceUpdate(BaseModel):
    nombre: str = Field(..., min_length=3, max_length=50, description="El nuevo nombre para el espacio de trabajo.")


# ===================================================================================
# --- API PARA GESTIÃ“N DE ESPACIOS DE TRABAJO ---
# ===================================================================================
@app.get("/workspaces", summary="Obtiene los espacios de trabajo del usuario", tags=["Espacios de Trabajo"])
async def get_workspaces(current_user: dict = Depends(get_current_user)):
    """
    Devuelve una lista de todos los espacios de trabajo pertenecientes
    al usuario autenticado. La autenticaciÃ³n es obligatoria.
    """
    try:
        user_email = current_user.get("email")
        
        workspaces_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo')
        # Ordenamos por el campo que ahora sabemos que existe y se actualiza
        query = workspaces_ref.order_by("fechaUltimoAcceso", direction=firestore.Query.DESCENDING)
        
        workspaces_list = []
        for doc in query.stream():
            workspace_data = doc.to_dict()
            workspace_data['id'] = doc.id
            
            # --- CONVERSIÃ“N DE FECHAS COMPLETA Y ROBUSTA ---
            # Iteramos sobre una lista de campos de fecha conocidos para convertirlos
            date_fields_to_convert = ['fechaCreacion', 'fechaUltimoAcceso', 'fechaModificacion']
            
            for field in date_fields_to_convert:
                if field in workspace_data and hasattr(workspace_data[field], 'isoformat'):
                    workspace_data[field] = workspace_data[field].isoformat()
            
            workspaces_list.append(workspace_data)

        # --- LÃ³gica para obtener los crÃ©ditos (sin cambios) ---
        creditos_restantes = current_user.get("creditos_restantes", 0)
        creditos_iniciales = current_user.get("creditos_iniciales", 50)
        creditos_usados = creditos_iniciales - creditos_restantes

        return JSONResponse(content={
            "workspaces": workspaces_list,
            "credits": {
                "used": creditos_usados,
                "remaining": creditos_restantes
            },
            "user": {
                "email": user_email
                # AquÃ­ podrÃ­as aÃ±adir otros datos del perfil si los necesitas en el futuro
            }
        })

    except Exception as e:
        print(f"ðŸ”¥ Error al obtener workspaces para el usuario {user_email}: {e}")
        raise HTTPException(status_code=500, detail="No se pudieron obtener los espacios de trabajo.")

@app.post("/workspaces", summary="Crea un nuevo espacio de trabajo (con validaciÃ³n de plan)", tags=["Espacios de Trabajo"])
async def create_workspace(
    workspace: WorkspaceCreate,
    current_user: dict = Depends(get_current_user)
):
    user_email = current_user.get("email")
    if not user_email:
        raise HTTPException(status_code=403, detail="No se pudo identificar al usuario desde el token.")

    try:
        user_plan = current_user.get("plan", "gratis")
        plan_config = PLANS_CONFIG.get(user_plan, PLANS_CONFIG["gratis"])
        limit = plan_config["workspace_limit"]
        
        workspaces_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo')
        
        # --- LÃ“GICA DE VALIDACIÃ“N DE LÃMITES (CON DEPURACIÃ“N) ---
        if limit != -1: # -1 significa ilimitado
            # Contamos los workspaces existentes
            existing_workspaces = list(workspaces_ref.stream())
            workspaces_count = len(existing_workspaces)
            
            # --- DEBUGGING LOGS ---
            print("\n--- DEBUG: VerificaciÃ³n de LÃ­mite de Workspaces ---")
            print(f"Usuario: {user_email}, Plan: {user_plan}")
            print(f"LÃ­mite del plan: {limit}")
            print(f"Workspaces existentes encontrados: {workspaces_count}")
            # --- FIN DEBUGGING ---

            if workspaces_count >= limit:
                # Si se alcanza el lÃ­mite, lanzamos el error 403 y la funciÃ³n debe detenerse aquÃ­.
                print(f"ðŸš« LÃ­mite alcanzado para {user_email}. Lanzando error 403.")
                raise HTTPException(
                    status_code=403, 
                    detail=f"Has alcanzado el lÃ­mite de {limit} espacios de trabajo para el plan '{plan_config['plan_name']}'. Considera mejorar tu plan."
                )
        
        # --- La creaciÃ³n solo ocurre si la validaciÃ³n anterior no lanzÃ³ un error ---
        print(f"âœ… LÃ­mite verificado. Procediendo a crear workspace para {user_email}.")
        new_workspace_data = {
            "nombre": workspace.nombre,
            "fechaCreacion": datetime.now(timezone.utc),
            "fechaUltimoAcceso": datetime.now(timezone.utc), # Inicializamos el campo de ordenamiento
            "isPinned": False,
            "auditorias_ejecutadas": 0
        }
        
        update_time, new_workspace_ref = workspaces_ref.add(new_workspace_data)
        
        # Preparamos la respuesta para que sea compatible con JSON
        response_data = new_workspace_data.copy()
        response_data['id'] = new_workspace_ref.id
        response_data['fechaCreacion'] = response_data['fechaCreacion'].isoformat()
        response_data['fechaUltimoAcceso'] = response_data['fechaUltimoAcceso'].isoformat()
        
        return JSONResponse(
            status_code=201,
            content={"message": "Espacio de trabajo creado exitosamente.", "workspace": response_data}
        )

    # --- MANEJO DE ERRORES CORREGIDO ---
    # 1. Capturamos nuestra propia excepciÃ³n HTTP primero y la relanzamos.
    except HTTPException as http_exc:
        raise http_exc
    # 2. Capturamos cualquier otro error inesperado despuÃ©s.
    except Exception as e:
        print(f"ðŸ”¥ Error inesperado al crear workspace para el usuario {user_email}: {e}")
        raise HTTPException(status_code=500, detail="No se pudo crear el espacio de trabajo debido a un error interno.")

@app.put("/workspaces/{workspace_id}", summary="Actualiza el nombre de un espacio de trabajo", tags=["Espacios de Trabajo"])
async def update_workspace(
    workspace_id: str,
    workspace_update: WorkspaceUpdate,
    current_user: dict = Depends(get_current_user)
):
    """
    Permite a un usuario autenticado renombrar uno de sus espacios de trabajo.
    """
    try:
        user_email = current_user.get("email")
        # La referencia al documento verifica implÃ­citamente la propiedad
        workspace_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo').document(workspace_id)
        
        # Verificamos que el workspace exista antes de intentar actualizarlo
        if not workspace_ref.get().exists:
            raise HTTPException(status_code=404, detail="El espacio de trabajo no fue encontrado o no te pertenece.")
            
        # Actualizamos solo el campo 'nombre'
        workspace_ref.update({"nombre": workspace_update.nombre})
        
        print(f"âœ… Workspace '{workspace_id}' renombrado a '{workspace_update.nombre}' para el usuario {user_email}.")
        return {"message": "Espacio de trabajo actualizado exitosamente.", "id": workspace_id, "nuevo_nombre": workspace_update.nombre}

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        print(f"ðŸ”¥ Error al actualizar workspace '{workspace_id}': {e}")
        raise HTTPException(status_code=500, detail="No se pudo actualizar el espacio de trabajo.")


@app.delete("/workspaces/{workspace_id}", summary="Elimina un espacio de trabajo", tags=["Espacios de Trabajo"])
async def delete_workspace(
    workspace_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    Permite a un usuario autenticado eliminar uno de sus espacios de trabajo.
    ADVERTENCIA: En esta versiÃ³n MVP, esto solo elimina el documento del workspace,
    pero las sub-colecciones (archivos, reportes) pueden quedar huÃ©rfanas.
    """
    try:
        user_email = current_user.get("email")
        workspace_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo').document(workspace_id)

        if not workspace_ref.get().exists:
            raise HTTPException(status_code=404, detail="El espacio de trabajo no fue encontrado o no te pertenece.")

        # Eliminamos el documento del espacio de trabajo
        workspace_ref.delete()
        
        print(f"âœ… Workspace '{workspace_id}' eliminado para el usuario {user_email}.")
        return {"message": "Espacio de trabajo eliminado exitosamente."}
        
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        print(f"ðŸ”¥ Error al eliminar workspace '{workspace_id}': {e}")
        raise HTTPException(status_code=500, detail="No se pudo eliminar el espacio de trabajo.")


@app.put("/workspaces/{workspace_id}/pin", summary="Fija o desfija un espacio de trabajo", tags=["Espacios de Trabajo"])
async def pin_workspace(
    workspace_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    Invierte el estado 'isPinned' de un espacio de trabajo.
    Esta es una funcionalidad Pro.
    """
    user_email = current_user.get("email")
         
    try:
        workspace_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo').document(workspace_id)
        workspace_doc = workspace_ref.get()
        
        if not workspace_doc.exists:
            raise HTTPException(status_code=404, detail="Espacio de trabajo no encontrado.")
        
        # Obtenemos el estado actual de 'isPinned' y lo invertimos
        current_pin_status = workspace_doc.to_dict().get("isPinned", False)
        new_pin_status = not current_pin_status
        
        workspace_ref.update({"isPinned": new_pin_status})
        
        return {"message": "Estado de fijado actualizado.", "id": workspace_id, "isPinned": new_pin_status}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"No se pudo actualizar el estado: {e}")


# --- NUEVO ENDPOINT PARA REGISTRAR EL ACCESO A UN WORKSPACE ---
@app.put("/workspaces/{workspace_id}/touch", summary="Actualiza la fecha de Ãºltimo acceso de un workspace", tags=["Espacios de Trabajo"])
async def touch_workspace(
    workspace_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    Actualiza el timestamp 'fechaUltimoAcceso' de un espacio de trabajo.
    Esta acciÃ³n se llama cada vez que un usuario entra a un espacio,
    para que la lista se pueda ordenar por "abiertos recientemente".
    """
    try:
        user_email = current_user.get("email")
        workspace_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo').document(workspace_id)
        
        # Usamos .update() para establecer o actualizar el campo
        workspace_ref.update({"fechaUltimoAcceso": datetime.now(timezone.utc)})
        
        return {"message": f"Timestamp de acceso actualizado para {workspace_id}."}

    except Exception as e:
        # No queremos que un fallo aquÃ­ rompa la experiencia del usuario,
        # asÃ­ que en lugar de un error 500, podrÃ­amos solo registrarlo.
        print(f"ðŸ”¥ Advertencia: No se pudo actualizar el timestamp de acceso para el workspace {workspace_id}: {e}")
        # Devolvemos una respuesta exitosa de todas formas para no bloquear al frontend.
        return {"message": "La operaciÃ³n continuÃ³ aunque no se pudo actualizar el timestamp."}


# ===================================================================================
# --- 4. NUEVOS ENDPOINTS PARA GESTIONAR LA ESTRATEGIA ---
# ===================================================================================

@app.get("/strategy/default", summary="Obtiene la estrategia de negocio por defecto", tags=["Estrategia"])
async def get_default_strategy():
    """Devuelve la configuraciÃ³n base recomendada para los parÃ¡metros de anÃ¡lisis."""
    return JSONResponse(content=DEFAULT_STRATEGY)

@app.get("/strategy", summary="Obtiene la estrategia global del usuario", tags=["Estrategia"])
async def get_global_strategy(current_user: dict = Depends(get_current_user)):
    """Devuelve la estrategia global guardada para el usuario autenticado."""
    return JSONResponse(content=current_user.get("estrategia_global", DEFAULT_STRATEGY))

@app.put("/strategy", summary="Actualiza la estrategia global del usuario", tags=["Estrategia"])
async def update_global_strategy(strategy_data: StrategyData, current_user: dict = Depends(get_current_user)):
    """Actualiza la estrategia global para el usuario autenticado."""
    user_email = current_user.get("email")
    user_ref = db.collection('usuarios').document(user_email)
    user_ref.update({"estrategia_global": strategy_data.dict()})
    return {"message": "Estrategia global actualizada."}

@app.get("/workspaces/{workspace_id}/strategy", summary="Obtiene la estrategia efectiva para un workspace", tags=["Estrategia"])
async def get_workspace_strategy(workspace_id: str, current_user: dict = Depends(get_current_user)):
    """
    Devuelve la estrategia para un workspace, aplicando la jerarquÃ­a:
    1. Estrategia personalizada del workspace (si existe).
    2. Estrategia global del usuario.
    """
    user_email = current_user.get("email")
    workspace_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo').document(workspace_id)
    workspace_doc = workspace_ref.get()

    if not workspace_doc.exists:
        raise HTTPException(status_code=404, detail="Espacio de trabajo no encontrado.")

    workspace_data = workspace_doc.to_dict()
    
    # LÃ³gica de cascada
    if "estrategia_personalizada" in workspace_data:
        return JSONResponse(content=workspace_data["estrategia_personalizada"])
    else:
        return JSONResponse(content=current_user.get("estrategia_global", DEFAULT_STRATEGY))

@app.put("/workspaces/{workspace_id}/strategy", summary="Guarda una estrategia personalizada para un workspace", tags=["Estrategia"])
async def save_workspace_strategy(workspace_id: str, strategy_data: StrategyData, current_user: dict = Depends(get_current_user)):
    """Guarda o actualiza la estrategia personalizada para un workspace especÃ­fico."""
    user_email = current_user.get("email")
    workspace_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo').document(workspace_id)
    workspace_ref.update({"estrategia_personalizada": strategy_data.dict()})
    return {"message": "Estrategia del espacio de trabajo guardada."}

@app.delete("/workspaces/{workspace_id}/strategy", summary="Restaura la estrategia de un workspace a la global", tags=["Estrategia"])
async def reset_workspace_strategy(workspace_id: str, current_user: dict = Depends(get_current_user)):
    """
    Elimina la estrategia personalizada de un workspace, haciendo que vuelva a
    heredar la estrategia global del usuario.
    """
    user_email = current_user.get("email")
    workspace_ref = db.collection('usuarios').document(user_email).collection('espacios_trabajo').document(workspace_id)
    workspace_ref.update({"estrategia_personalizada": firestore.DELETE_FIELD})
    return {"message": "Estrategia del espacio de trabajo restaurada a la global."}




# # ===================================================================================
# # --- NUEVOS ENDPOINTS PARA GESTIONAR LA ESTRATEGIA ---
# # ===================================================================================

@app.get("/sessions/{session_id}/strategy", summary="Obtiene la estrategia guardada para una sesiÃ³n", tags=["Estrategia"])
async def get_session_strategy(session_id: str):
    """
    Devuelve la configuraciÃ³n de estrategia personalizada que estÃ¡ guardada
    para una sesiÃ³n especÃ­fica en Firestore.
    """
    try:
        session_ref = db.collection('sesiones_anonimas').document(session_id)
        session_doc = session_ref.get()

        if not session_doc.exists:
            # Si la sesiÃ³n no existe, es un error del cliente.
            raise HTTPException(status_code=404, detail="La sesiÃ³n no existe.")

        # Buscamos el campo 'estrategia' dentro del documento de la sesiÃ³n.
        strategy_data = session_doc.to_dict().get("estrategia")

        if not strategy_data:
            # Como plan B, si por alguna razÃ³n el campo no existe,
            # devolvemos la estrategia por defecto para que la app no se rompa.
            print(f"Advertencia: No se encontrÃ³ estrategia para la sesiÃ³n {session_id}. Devolviendo default.")
            return JSONResponse(content=DEFAULT_STRATEGY)

        return JSONResponse(content=strategy_data)

    except HTTPException as http_exc:
        # Relanzamos los errores HTTP que nosotros mismos generamos
        raise http_exc
    except Exception as e:
        print(f"ðŸ”¥ Error al obtener la estrategia para la sesiÃ³n {session_id}: {e}")
        raise HTTPException(status_code=500, detail=f"No se pudo obtener la estrategia: {e}")


@app.post("/upload-file", summary="Sube, registra y cachea los filtros de un archivo", tags=["Archivos"])
async def upload_file(
    # --- ParÃ¡metros de contexto (sin cambios) ---
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: Optional[str] = Header(None, alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),
    # --- ParÃ¡metros de la peticiÃ³n (sin cambios) ---
    tipo_archivo: Literal['inventario', 'ventas'] = Form(...),
    file: UploadFile = File(...)
):
    user_id = None
    session_id_to_use = None

    # --- LÃ³gica de DeterminaciÃ³n de Contexto (sin cambios) ---
    if current_user:
        # if not workspace_id:
        #     raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        user_id = current_user['email']
        print(f"Contexto de carga: Usuario Registrado ({user_id}), Workspace ({workspace_id})")
    elif X_Session_ID:
        session_id_to_use = X_Session_ID
        print(f"Contexto de carga: SesiÃ³n AnÃ³nima ({session_id_to_use})")
    else:
        raise HTTPException(status_code=401, detail="No se proporcionÃ³ autenticaciÃ³n ni ID de sesiÃ³n.")

    contents = await file.read()
    df = None
    last_error = None

    # Lista de configuraciones a intentar, en orden de probabilidad
    configs_to_try = [
        {'sep': ',', 'encoding': 'utf-8', 'skiprows': 0},
        {'sep': ',', 'encoding': 'latin1', 'skiprows': 0},
        {'sep': ';', 'encoding': 'utf-8', 'skiprows': 0},
        {'sep': ';', 'encoding': 'latin1', 'skiprows': 0},
        # Como plan B, intentamos leer sin saltar ninguna fila, por si el formato cambia
        {'sep': ',', 'encoding': 'utf-8'},
    ]

    for config in configs_to_try:
        try:
            # Creamos un nuevo flujo de bytes en cada intento para empezar desde el principio
            df = pd.read_csv(io.BytesIO(contents), **config)
            print(f"âœ… Archivo leÃ­do exitosamente con la configuraciÃ³n: {config}")
            break # Si tiene Ã©xito, salimos del bucle
        except Exception as e:
            last_error = e
            # Si falla, simplemente continuamos con la siguiente configuraciÃ³n
            continue
    
    # Si despuÃ©s de todos los intentos no se pudo leer, lanzamos el error final
    if df is None:
        raise HTTPException(
            status_code=400,
            detail=f"No se pudo leer el archivo CSV. Verifique su formato. Error final: {last_error}"
        )

    # El resto de tu lÃ³gica no cambia
    df.columns = df.columns.str.strip()


    try:
        metadata = extraer_metadatos_df(df, tipo_archivo)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error al procesar el contenido del archivo: {e}")
    
    try:
        # --- Guardado en Firebase y Registro ---
        now = datetime.now(timezone.utc)
        timestamp_str = now.strftime('%Y-%m-%d_%H%M%S')
        
        ruta_storage = upload_to_storage(
            user_id=user_id,
            workspace_id=workspace_id,
            session_id=session_id_to_use,
            file_contents=contents,
            tipo_archivo=tipo_archivo,
            original_filename=file.filename,
            content_type=file.content_type,
            timestamp_str=timestamp_str
        ) # Tu llamada a upload_to_storage
        
        file_id = f"{timestamp_str}_{tipo_archivo}"

        log_file_upload_in_firestore(
            user_id=user_id,
            workspace_id=workspace_id,
            session_id=session_id_to_use,
            file_id=file_id,
            tipo_archivo=tipo_archivo,
            nombre_original=file.filename,
            ruta_storage=ruta_storage,
            metadata=metadata,
            timestamp_obj=now
        )

        # print(f"âœ… Metadata en '{metadata}'.")

        # --- INICIO DE LA NUEVA LÃ“GICA DE CACHÃ‰ PARA INVENTARIO---
        # Si la carga es de un usuario registrado y el archivo es un inventario...
        if user_id and workspace_id and tipo_archivo == 'inventario':
            print(f"Detectado archivo de inventario para usuario registrado. Cacheando filtros...")
            
            # Obtenemos las listas de filtros desde los metadatos que ya extrajimos
            filtros_a_guardar = {
                "categorias": metadata.get("lista_completa_categorias", []),
                "marcas": metadata.get("lista_completa_marcas", [])
            }

            # Obtenemos la referencia al documento del ESPACIO DE TRABAJO
            workspace_ref = db.collection('usuarios').document(user_id).collection('espacios_trabajo').document(workspace_id)
            
            # Actualizamos el documento del workspace con los filtros disponibles
            workspace_ref.update({"filtros_disponibles": filtros_a_guardar})
            
            print(f"âœ… Filtros cacheados exitosamente en el workspace '{workspace_id}'.")
        # --- FIN DE LA NUEVA LÃ“GICA DE CACHÃ‰ ---

        # --- INICIO DE LA NUEVA LÃ“GICA DE CACHÃ‰ PARA VENTAS---
        # Si la carga es de un usuario registrado y el archivo es un inventario...
        if user_id and workspace_id and tipo_archivo == 'ventas':
            print(f"Detectado archivo de ventas para usuario registrado. Cacheando fechas...")
            
            # Obtenemos las listas de filtros desde los metadatos que ya extrajimos
            fechas_a_guardar = {
                "min_date": metadata["fecha_primera_venta"].isoformat(),
                "max_date": metadata["fecha_ultima_venta"].isoformat()
            }

            # Obtenemos la referencia al documento del ESPACIO DE TRABAJO
            workspace_ref = db.collection('usuarios').document(user_id).collection('espacios_trabajo').document(workspace_id)
            
            # Actualizamos el documento del workspace con los filtros disponibles
            workspace_ref.update({"fechas_disponibles": fechas_a_guardar})
            
            print(f"âœ… Fechas cacheados exitosamente en el workspace '{workspace_id}'.")
        # --- FIN DE LA NUEVA LÃ“GICA DE CACHÃ‰ ---

        # --- INICIO DE LA NUEVA LÃ“GICA DE CACHÃ‰ PARA VENTAS - ANONIMAS ---
        # Si la carga es de un usuario registrado y el archivo es un inventario...
        if session_id_to_use and tipo_archivo == 'ventas':
            print(f"Detectado archivo de ventas para usuario anonimo. Cacheando fechas...")
            
            # Obtenemos las listas de filtros desde los metadatos que ya extrajimos
            fechas_a_guardar = {
                "min_date": metadata["fecha_primera_venta"].isoformat(),
                "max_date": metadata["fecha_ultima_venta"].isoformat()
            }

            # Obtenemos la referencia al documento del ESPACIO DE TRABAJO
            workspace_anonimo_ref = db.collection('sesiones_anonimas').document(session_id_to_use)
            
            # Actualizamos el documento del workspace con los filtros disponibles
            workspace_anonimo_ref.update({"fechas_disponibles": fechas_a_guardar})
            
            print(f"âœ… Fechas cacheados exitosamente para anonimo en '{session_id_to_use}'.")
        # --- FIN DE LA NUEVA LÃ“GICA DE CACHÃ‰ ---

        response_content = {}

        # Devolvemos los metadatos extraÃ­dos para que la UI se actualice al instante
        if tipo_archivo == 'ventas' and metadata.get("fecha_primera_venta"):
            response_content["date_range_bounds"] = {
                "min_date": metadata["fecha_primera_venta"].isoformat(),
                "max_date": metadata["fecha_ultima_venta"].isoformat()
            }
        

        # Si es un inventario (para cualquier tipo de usuario), devolvemos los filtros para uso inmediato
        if tipo_archivo == 'inventario':
            response_content["available_filters"] = {
                "categorias": metadata.get("lista_completa_categorias", []),
                "marcas": metadata.get("lista_completa_marcas", [])
            }

        # --- CAMBIO CLAVE: Convertimos las fechas en los metadatos ANTES de guardarlos ---
        if 'fecha_primera_venta' in metadata and hasattr(metadata['fecha_primera_venta'], 'isoformat'):
            metadata['fecha_primera_venta'] = metadata['fecha_primera_venta'].isoformat()
        if 'fecha_ultima_venta' in metadata and hasattr(metadata['fecha_ultima_venta'], 'isoformat'):
            metadata['fecha_ultima_venta'] = metadata['fecha_ultima_venta'].isoformat()

        # La construcciÃ³n de la respuesta no cambia
        response_content = {
            "message": f"Archivo de {tipo_archivo} subido exitosamente.",
            "file_id": file_id,
            "tipo_archivo": tipo_archivo,
            "nombre_original": file.filename,
            "metadata": metadata
        }

        # --- CAMBIO CLAVE: AÃ±adimos el rango de fechas a la respuesta ---
        if tipo_archivo == 'ventas':
            if metadata.get("fecha_primera_venta") and metadata.get("fecha_ultima_venta"):
                response_content["date_range_bounds"] = {
                    "min_date": metadata["fecha_primera_venta"],
                    "max_date": metadata["fecha_ultima_venta"]
                }

        # Si es un inventario (para cualquier tipo de usuario), devolvemos los filtros para uso inmediato
        if tipo_archivo == 'inventario':
            response_content["available_filters"] = {
                "categorias": metadata.get("lista_completa_categorias", []),
                "marcas": metadata.get("lista_completa_marcas", [])
            }
        
        return JSONResponse(content=response_content)

    except Exception as e:
        print(f"ðŸ”¥ Error en la fase de guardado: {e}")
        raise HTTPException(status_code=500, detail="OcurriÃ³ un error al guardar el archivo en el servidor.")


# ===================================================================================
# --- TUS ENDPOINTS EXISTENTES (EJEMPLO) ---
# ===================================================================================
# Por ahora, mantenemos tu endpoint /extract-metadata como estaba,
# ya que lo refactorizaremos en el siguiente paso para usar el nuevo flujo.

@app.post("/extract-metadata", summary="Extrae metadatos (categorÃ­as, marcas) de un archivo de inventario", tags=["Utilidades"])
async def extract_metadata(
    inventario: UploadFile = File(..., description="Archivo CSV con datos de inventario."),
    X_Session_ID: str = Header(..., alias="X-Session-ID", description="ID de sesiÃ³n anÃ³nima Ãºnico del cliente.")
):
    if not X_Session_ID:
            raise HTTPException(status_code=400, detail="La cabecera X-Session-ID es requerida.")
    """
    Lee un archivo de inventario y devuelve una lista de todas las categorÃ­as y marcas Ãºnicas.
    Este endpoint es robusto e intenta leer el CSV con diferentes codificaciones y separadores.
    """

    inventory_contents = await inventario.read()

    # --- LÃ“GICA DE LECTURA ROBUSTA ---
    try:
        # Intento 1: La configuraciÃ³n mÃ¡s comÃºn (UTF-8, separado por comas)
        df_inventario = pd.read_csv(io.BytesIO(inventory_contents), sep=',')
    except (UnicodeDecodeError, pd.errors.ParserError):
        try:
            # Intento 2: CodificaciÃ³n Latin-1 (muy comÃºn en Excel de Windows/EspaÃ±ol)
            print("Intento 1 (UTF-8, coma) fallÃ³. Reintentando con latin-1 y coma.")
            df_inventario = pd.read_csv(io.BytesIO(inventory_contents), sep=',', encoding='latin1')
        except (UnicodeDecodeError, pd.errors.ParserError):
            try:
                # Intento 3: CodificaciÃ³n UTF-8 con punto y coma como separador
                print("Intento 2 (latin-1, coma) fallÃ³. Reintentando con UTF-8 y punto y coma.")
                df_inventario = pd.read_csv(io.BytesIO(inventory_contents), sep=';', encoding='utf-8')
            except Exception as e:
                # Si todos los intentos fallan, entonces sÃ­ lanzamos el error.
                print(f"Todos los intentos de lectura de CSV fallaron. Error final: {e}")
                raise HTTPException(
                    status_code=400, 
                    detail=f"No se pudo leer el archivo CSV. Verifique que estÃ© delimitado por comas o punto y coma y tenga una codificaciÃ³n estÃ¡ndar (UTF-8 o Latin-1). Error: {e}"
                )

    # --- 2. ExtracciÃ³n de Metadatos del DataFrame ---
    metadata_inventario = extraer_metadatos_df(df_inventario, 'inventario')


    # --- 3. Guardado AsÃ­ncrono en Firebase ---
    # Esto puede correr en segundo plano si se desea, pero por simplicidad lo hacemos secuencial.
    try:
        now = datetime.now(timezone.utc)

        timestamp_str = now.strftime('%Y-%m-%d_%H%M%S')
        
        inventory_file_path = upload_to_storage(
            session_id=X_Session_ID,
            file_contents=inventory_contents,
            tipo_archivo='inventario', # <--- Le indicamos que es un archivo de inventario
            original_filename=inventario.filename, # Pasamos el nombre para obtener la extensiÃ³n
            content_type=inventario.content_type,
            timestamp_str=timestamp_str
        )
        
        log_analysis_in_firestore(
            session_id=X_Session_ID,
            report_name="ExtractMetadata",
            timestamp_obj=now,
            inventory_path=inventory_file_path,
            metadata_inventario=metadata_inventario
        )
    except Exception as e:
        # Advertencia: No detenemos el flujo si falla el guardado, el usuario aÃºn
        # necesita su respuesta. PodrÃ­amos loggear este error internamente.
        print(f"ADVERTENCIA: FallÃ³ el guardado en Firebase para la sesiÃ³n {X_Session_ID}: {e}")

    # --- 4. Devolver la Respuesta al Cliente ---
    # Devolvemos los metadatos que el frontend necesita para los filtros.
    categorias = metadata_inventario.get("preview_categorias", [])
    marcas = [m for m in df_inventario['Marca'].dropna().unique().tolist() if m] if 'Marca' in df_inventario.columns else []

    return JSONResponse(content={
        "categorias_disponibles": categorias,
        "marcas_disponibles": sorted(marcas)
    })



@app.post("/abc", summary="Realizar AnÃ¡lisis ABC", tags=["AnÃ¡lisis"])
async def upload_csvs_abc_analysis(
    # Inyectamos el request para el logging
    request: Request, 
    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: str = Header(..., alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),

    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...),
    criterio_abc: str = Form(..., description="Criterio para el anÃ¡lisis ABC: 'ingresos', 'unidades', 'margen', 'combinado'.", examples=["ingresos"]),
    periodo_abc: int = Form(..., description="PerÃ­odo de anÃ¡lisis en meses (0 para todo el historial, ej: 3, 6, 12).", examples=[6]),

    # --- NUEVO: Recibimos los scores de la estrategia ---
    score_ventas: Optional[int] = Form(None),
    score_ingreso: Optional[int] = Form(None),
    score_margen: Optional[int] = Form(None),
    incluir_solo_categorias: str = Form("", description="String de categorÃ­as separadas por comas."),
    incluir_solo_marcas: str = Form("", description="String de marcas separadas por comas."),
    filtro_skus_json: Optional[str] = Form(None)
):
    user_id = None
    
    # --- LÃ“GICA DE DETERMINACIÃ“N DE CONTEXTO ---
    if current_user:
        # CASO 1: Usuario Registrado
        if not workspace_id:
            raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        user_id = current_user['email']
        print(f"PeticiÃ³n de usuario registrado: {user_id} para workspace: {workspace_id}")
        # Para el logging y descarga, pasamos el contexto del usuario
        log_session_id = None # No usamos el ID de sesiÃ³n anÃ³nima
    
    elif X_Session_ID:
        # CASO 2: Usuario AnÃ³nimo
        print(f"PeticiÃ³n de usuario anÃ³nimo: {X_Session_ID}")
        log_session_id = X_Session_ID
    else:
        # CASO 3: No hay identificador, denegamos el acceso
        raise HTTPException(status_code=401, detail="No se proporcionÃ³ autenticaciÃ³n ni ID de sesiÃ³n.")

    # pesos_combinado_dict = None
    # if criterio_abc.lower() == "combinado":
    #     pesos_combinado_dict = {
    #         "ingresos": peso_ingresos,
    #         "margen": peso_margen,
    #         "unidades": score_ventas
    #     }
    
    # Preparamos el diccionario de parÃ¡metros para la funciÃ³n de lÃ³gica
    try:
        # Parseamos los strings JSON para convertirlos en listas de Python
        categorias_list = json.loads(incluir_solo_categorias) if incluir_solo_categorias else None
        marcas_list = json.loads(incluir_solo_marcas) if incluir_solo_marcas else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtros (categorÃ­as/marcas) invÃ¡lido. Se esperaba un string JSON.")

    try:
        filtro_skus = json.loads(filtro_skus_json) if filtro_skus_json else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro de SKUs invÃ¡lido.")

    processing_params = {
        "filtro_skus": filtro_skus,
        "criterio_abc": criterio_abc.lower(),
        "periodo_abc": periodo_abc,
        # "pesos_combinado": pesos_combinado_dict,
        "criterio_abc": criterio_abc,
        "periodo_abc": periodo_abc,
        "score_ventas": score_ventas,
        "score_ingreso": score_ingreso,
        "score_margen": score_margen,
        "filtro_categorias": categorias_list,
        "filtro_marcas": marcas_list
    }

    full_params_for_logging = dict(await request.form())

    # Llamamos a la funciÃ³n manejadora central
    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id,
        report_key="ReporteABC",
        processing_function=process_csv_abc, # Pasamos la funciÃ³n de lÃ³gica como argumento
        processing_params=processing_params,
        output_filename="reporte_abc.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=log_session_id
    )


@app.post("/rotacion-general-estrategico") # Endpoint renombrado para mayor claridad
async def run_analisis_estrategico_rotacion(
    # Inyectamos el request para el logging
    request: Request, 
    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: str = Header(..., alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),
    
    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...),

    # --- ParÃ¡metros BÃ¡sicos (Controles Principales) ---
    dias_analisis_ventas_recientes: Optional[int] = Form(30, description="Ventana principal de anÃ¡lisis en dÃ­as. Ej: 30, 60, 90."),
    sort_by: str = Form('Importancia_Dinamica', description="Columna para ordenar el resultado."),
    sort_ascending: bool = Form(False, description="True para orden ascendente (ej: ver lo mÃ¡s bajo en cobertura)."),
    filtro_categorias_json: Optional[str] = Form(None, description='Filtra por una o mÃ¡s categorÃ­as. Formato JSON: \'["TornillerÃ­a"]\''),
    filtro_marcas_json: Optional[str] = Form(None, description='Filtra por una o mÃ¡s marcas. Formato JSON: \'["Marca A"]\''),
    min_importancia: Optional[float] = Form(None, description="Filtro para ver productos con importancia >= a este valor (0 a 1)."),
    max_dias_cobertura: Optional[float] = Form(None, description="Filtro para encontrar productos con bajo stock (cobertura <= X dÃ­as)."),
    min_dias_cobertura: Optional[float] = Form(None, description="Filtro para encontrar productos con sobre-stock (cobertura >= X dÃ­as)."),

    # --- ParÃ¡metros Avanzados (Ajustes del Modelo) ---
    dias_analisis_ventas_general: Optional[int] = Form(None, description="Ventana secundaria de anÃ¡lisis para productos sin ventas recientes."),
    umbral_sobre_stock_dias: int = Form(180, description="DÃ­as a partir de los cuales un producto se considera 'Sobre-stock'."),
    umbral_stock_bajo_dias: int = Form(15, description="DÃ­as por debajo de los cuales un producto se considera con 'Stock Bajo'."),
    # pesos_importancia_json: Optional[str] = Form(None, description='(Avanzado) Redefine los pesos del Ãndice de Importancia. Formato JSON.')

    filtro_bcg_json: Optional[str] = Form(None),
    min_valor_stock: Optional[float] = Form(0.0),

    # --- NUEVO: Recibimos los scores de la estrategia desde el frontend ---
    score_ventas: int = Form(...),
    score_ingreso: int = Form(...),
    score_margen: int = Form(...),
    score_dias_venta: int = Form(...),

    filtro_skus_json: Optional[str] = Form(None)
):
    user_id = None
    
    # --- LÃ“GICA DE DETERMINACIÃ“N DE CONTEXTO ---
    if current_user:
        # CASO 1: Usuario Registrado
        if not workspace_id:
            raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        user_id = current_user['email']
        print(f"PeticiÃ³n de usuario registrado: {user_id} para workspace: {workspace_id}")
        # Para el logging y descarga, pasamos el contexto del usuario
        log_session_id = None # No usamos el ID de sesiÃ³n anÃ³nima
    
    elif X_Session_ID:
        # CASO 2: Usuario AnÃ³nimo
        print(f"PeticiÃ³n de usuario anÃ³nimo: {X_Session_ID}")
        log_session_id = X_Session_ID
    else:
        # CASO 3: No hay identificador, denegamos el acceso
        raise HTTPException(status_code=401, detail="No se proporcionÃ³ autenticaciÃ³n ni ID de sesiÃ³n.")

    # --- 2. Procesar ParÃ¡metros Complejos desde JSON ---
    filtro_categorias = json.loads(filtro_categorias_json) if filtro_categorias_json else None
    filtro_marcas = json.loads(filtro_marcas_json) if filtro_marcas_json else None
    filtro_bcg_list = json.loads(filtro_bcg_json) if filtro_bcg_json else None
    # (Se podrÃ­a aÃ±adir un try-except mÃ¡s robusto aquÃ­ si se desea)

    # pesos_importancia = json.loads(pesos_importancia_json) if pesos_importancia_json else None
    # 1. Sumamos los scores recibidos
    total_scores = score_ventas + score_ingreso + score_margen + score_dias_venta

    # 2. Calculamos los pesos prorrateados, manejando el caso de divisiÃ³n por cero
    if total_scores == 0:
        # Si todos los scores son 0, asignamos un peso equitativo
        pesos_calculados = {'ventas': 0.25, 'ingreso': 0.25, 'margen': 0.25, 'dias_venta': 0.25}
    else:
        pesos_calculados = {
            'ventas': score_ventas / total_scores,
            'ingreso': score_ingreso / total_scores,
            'margen': score_margen / total_scores,
            'dias_venta': score_dias_venta / total_scores
        }
    
    try:
        filtro_skus = json.loads(filtro_skus_json) if filtro_skus_json else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro de SKUs invÃ¡lido.")

    # Preparamos el diccionario de parÃ¡metros para la funciÃ³n de lÃ³gica
    processing_params = {
        "filtro_skus": filtro_skus,
        "pesos_importancia": pesos_calculados,
        "dias_analisis_ventas_recientes": dias_analisis_ventas_recientes,
        "dias_analisis_ventas_general": dias_analisis_ventas_general,
        "umbral_sobre_stock_dias": umbral_sobre_stock_dias,
        "umbral_stock_bajo_dias": umbral_stock_bajo_dias,
        "filtro_categorias": filtro_categorias,
        "filtro_marcas": filtro_marcas,
        "min_importancia": min_importancia,
        "max_dias_cobertura": max_dias_cobertura,
        "min_dias_cobertura": min_dias_cobertura,
        "sort_by": sort_by,
        "sort_ascending": sort_ascending,
        "filtro_bcg": filtro_bcg_list,
        "min_valor_stock": min_valor_stock,
    }

    full_params_for_logging = dict(await request.form())

    # Llamamos a la funciÃ³n manejadora central
    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id,
        report_key="ReporteAnalisisEstrategicoRotacion",
        processing_function=process_csv_analisis_estrategico_rotacion, # Pasamos la funciÃ³n de lÃ³gica como argumento
        processing_params=processing_params,
        output_filename="ReporteAnalisisEstrategicoRotacion.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=log_session_id
    )


@app.post("/diagnostico-stock-muerto", summary="Genera el reporte de DiagnÃ³stico de Stock Muerto", tags=["Reportes"])
async def diagnostico_stock_muerto(
    # Inyectamos el request para el logging
    request: Request, 
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: Optional[str] = Header(None, alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),

    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...),

    # --- Recibimos los nuevos parÃ¡metros del formulario ---
    dias_sin_venta_muerto: int = Form(180),
    umbral_valor_stock: float = Form(0.0),
    ordenar_por: str = Form(...),
    incluir_solo_categorias: str = Form("", description="String de categorÃ­as separadas por comas."),
    incluir_solo_marcas: str = Form("", description="String de marcas separadas por comas."),
    filtro_skus_json: Optional[str] = Form(None)
):
    # --- LÃ“GICA DE DETERMINACIÃ“N DE CONTEXTO ---
    # --- DeterminaciÃ³n del Contexto ---
    # user_id = current_user['email'] if current_user else None
    user_id = None
    
    # --- LÃ“GICA DE DETERMINACIÃ“N DE CONTEXTO ---
    if current_user:
        # CASO 1: Usuario Registrado
        if not workspace_id:
            raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        user_id = current_user['email']
        print(f"PeticiÃ³n de usuario registrado: {user_id} para workspace: {workspace_id}")
        # Para el logging y descarga, pasamos el contexto del usuario
        log_session_id = None # No usamos el ID de sesiÃ³n anÃ³nima
    
    elif X_Session_ID:
        # CASO 2: Usuario AnÃ³nimo
        print(f"PeticiÃ³n de usuario anÃ³nimo: {X_Session_ID}")
        log_session_id = X_Session_ID
    else:
        # CASO 3: No hay identificador, denegamos el acceso
        raise HTTPException(status_code=401, detail="No se proporcionÃ³ autenticaciÃ³n ni ID de sesiÃ³n.")

    try:
        # Parseamos los strings JSON para convertirlos en listas de Python
        categorias_list = json.loads(incluir_solo_categorias) if incluir_solo_categorias else None
        marcas_list = json.loads(incluir_solo_marcas) if incluir_solo_marcas else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtros (categorÃ­as/marcas) invÃ¡lido. Se esperaba un string JSON.")

    try:
        filtro_skus = json.loads(filtro_skus_json) if filtro_skus_json else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro de SKUs invÃ¡lido.")

    processing_params = {
        "filtro_skus": filtro_skus,
        "dias_sin_venta_muerto": dias_sin_venta_muerto,
        "umbral_valor_stock": umbral_valor_stock,
        "ordenar_por": ordenar_por,
        "incluir_solo_categorias": categorias_list,
        "incluir_solo_marcas": marcas_list
    }

    full_params_for_logging = dict(await request.form())

    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id,
        report_key="ReporteDiagnosticoStockMuerto",
        processing_function=procesar_stock_muerto,
        processing_params=processing_params,
        output_filename="Diagnostico_Stock_Muerto.xlsx",
        # --- Pasamos el contexto correcto al manejador ---
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=log_session_id,
    )

@app.post("/reporte-maestro-inventario")
async def generar_reporte_maestro_endpoint(
    # Inyectamos el request para el logging
    request: Request, 
    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: str = Header(..., alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),

    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...),
    
    # --- ParÃ¡metros para el AnÃ¡lisis ABC (con valores por defecto) ---
    criterio_abc: str = Form("margen", description="Criterio para el anÃ¡lisis ABC: 'ingresos', 'unidades', 'margen', o 'combinado'."),
    periodo_abc: int = Form(6, description="NÃºmero de meses hacia atrÃ¡s para el anÃ¡lisis ABC."),
    
    ordenar_por: str = Form("prioridad"),
    incluir_solo_categorias: str = Form("", description="String de categorÃ­as separadas por comas."),
    incluir_solo_marcas: str = Form("", description="String de marcas separadas por comas."),
    score_ventas: int = Form(...),
    score_ingreso: int = Form(...),
    score_margen: int = Form(...),
    # score_dias_venta: int = Form(...),

    # --- ParÃ¡metros Opcionales para el AnÃ¡lisis de Salud ---
    meses_analisis_salud: Optional[int] = Form(None, description="Meses para analizar ventas recientes en el diagnÃ³stico de salud."),
    dias_sin_venta_muerto: Optional[int] = Form(None, description="Umbral de dÃ­as para clasificar un producto como 'Stock Muerto'."),

    filtro_skus_json: Optional[str] = Form(None)
):
    user_id = None
    
    # --- LÃ“GICA DE DETERMINACIÃ“N DE CONTEXTO ---
    if current_user:
        # CASO 1: Usuario Registrado
        if not workspace_id:
            raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        user_id = current_user['email']
        print(f"PeticiÃ³n de usuario registrado: {user_id} para workspace: {workspace_id}")
        # Para el logging y descarga, pasamos el contexto del usuario
        log_session_id = None # No usamos el ID de sesiÃ³n anÃ³nima
    
    elif X_Session_ID:
        # CASO 2: Usuario AnÃ³nimo
        print(f"PeticiÃ³n de usuario anÃ³nimo: {X_Session_ID}")
        log_session_id = X_Session_ID
    else:
        # CASO 3: No hay identificador, denegamos el acceso
        raise HTTPException(status_code=401, detail="No se proporcionÃ³ autenticaciÃ³n ni ID de sesiÃ³n.")

    try:
        # Parseamos los strings JSON para convertirlos en listas de Python
        categorias_list = json.loads(incluir_solo_categorias) if incluir_solo_categorias else None
        marcas_list = json.loads(incluir_solo_marcas) if incluir_solo_marcas else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtros (categorÃ­as/marcas) invÃ¡lido. Se esperaba un string JSON.")

    try:
        filtro_skus = json.loads(filtro_skus_json) if filtro_skus_json else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro de SKUs invÃ¡lido.")


    # 1. Preparamos el diccionario de parÃ¡metros para la funciÃ³n de lÃ³gica
    processing_params = {
        "filtro_skus": filtro_skus,
        "criterio_abc": criterio_abc,
        "periodo_abc": periodo_abc,
        # "pesos_combinado": pesos_combinado,
        "score_ventas": score_ventas,
        "score_ingreso": score_ingreso,
        "score_margen": score_margen,
        # "score_dias_venta": score_dias_venta,
        "meses_analisis_salud": meses_analisis_salud,
        "dias_sin_venta_muerto": dias_sin_venta_muerto,
        "ordenar_por": ordenar_por,
        "incluir_solo_categorias": categorias_list,
        "incluir_solo_marcas": marcas_list
    }

    full_params_for_logging = dict(await request.form())

    # 2. Llamamos a la funciÃ³n manejadora central con toda la informaciÃ³n
    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id,
        report_key="ReporteMaestro", # La clave Ãºnica que definimos en la configuraciÃ³n
        processing_function=generar_reporte_maestro_inventario, # Tu funciÃ³n de lÃ³gica real
        # processing_function=lambda df_v, df_i, **kwargs: df_i.head(10), # SimulaciÃ³n
        processing_params=processing_params,
        output_filename="ReporteMaestro.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=log_session_id
    )


@app.post("/reporte-puntos-alerta-stock", summary="RecomendaciÃ³n Puntos de Alerta de Stock", tags=["AnÃ¡lisis"])
async def reporte_puntos_alerta_stock(
    # Inyectamos el request para el logging
    request: Request, 
    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: str = Header(..., alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),
    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...),
    lead_time_dias: int = Form(7.0),
    dias_seguridad_base: int = Form(0),
    factor_importancia_seguridad: float = Form(1.12),
    ordenar_por: str = Form("Diferencia_vs_Alerta_Minima"),
    excluir_sin_ventas: str = Form("true", description="String 'true' o 'false' para excluir productos sin ventas."),
    filtro_categorias_json: Optional[str] = Form(None),
    filtro_marcas_json: Optional[str] = Form(None)
):
    user_id = None
    
    # --- LÃ“GICA DE DETERMINACIÃ“N DE CONTEXTO ---
    if current_user:
        # CASO 1: Usuario Registrado
        if not workspace_id:
            raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        user_id = current_user['email']
        print(f"PeticiÃ³n de usuario registrado: {user_id} para workspace: {workspace_id}")
        # Para el logging y descarga, pasamos el contexto del usuario
        log_session_id = None # No usamos el ID de sesiÃ³n anÃ³nima
    
    elif X_Session_ID:
        # CASO 2: Usuario AnÃ³nimo
        print(f"PeticiÃ³n de usuario anÃ³nimo: {X_Session_ID}")
        log_session_id = X_Session_ID
    else:
        # CASO 3: No hay identificador, denegamos el acceso
        raise HTTPException(status_code=401, detail="No se proporcionÃ³ autenticaciÃ³n ni ID de sesiÃ³n.")

    try:
        excluir_bool = excluir_sin_ventas.lower() == 'true'
        filtro_categorias = json.loads(filtro_categorias_json) if filtro_categorias_json else None
        filtro_marcas = json.loads(filtro_marcas_json) if filtro_marcas_json else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro invÃ¡lido.")


    # Preparamos el diccionario de parÃ¡metros para la funciÃ³n de lÃ³gica
    processing_params = {
        # ParÃ¡metros de periodos para anÃ¡lisis de ventas
        # dias_analisis_ventas_recientes=30, #(P/FRONTEND) Anteriormente dias_importancia
        # dias_analisis_ventas_general=240,   #(P/FRONTEND) Anteriormente dias_promedio
        "peso_ventas_historicas": 0.6,
        # ParÃ¡metros para cÃ¡lculo de Stock Ideal
        "dias_cobertura_ideal_base": 10,
        "coef_importancia_para_cobertura_ideal": 0.05, # e.g., 0.25 (0 a 1), aumenta dÃ­as de cobertura ideal por importancia
        "coef_rotacion_para_stock_ideal": 0.10,       # e.g., 0.2 (0 a 1), aumenta stock ideal por rotaciÃ³n
        "coef_rotacion_para_stock_minimo": 0.15,
        # ParÃ¡metros para Pedido MÃ­nimo
        "dias_cubrir_con_pedido_minimo": 5, #(P/FRONTEND) DÃ­as de venta que un pedido mÃ­nimo deberÃ­a cubrir
        "coef_importancia_para_pedido_minimo": 0.1, # e.g., 0.5 (0 a 1), escala el pedido mÃ­nimo por importancia
        # Otros parÃ¡metros de comportamiento
        "importancia_minima_para_redondeo_a_1": 0.1, # e.g. 0.1, umbral de importancia para redondear pedidos pequeÃ±os a 1
        "incluir_productos_pasivos": True,
        "cantidad_reposicion_para_pasivos": 1, # e.g., 1 o 2, cantidad a reponer para productos pasivos
        "excluir_productos_sin_sugerencia_ideal": False, # Filtro para el resultado final
        # --- NUEVOS PARÃMETROS PARA EL PUNTO DE ALERTA ---
        "lead_time_dias": lead_time_dias,
        "dias_seguridad_base": dias_seguridad_base,
        "factor_importancia_seguridad": factor_importancia_seguridad,
        "ordenar_por": ordenar_por,
        "excluir_sin_ventas": excluir_bool,
        "filtro_categorias": filtro_categorias,
        "filtro_marcas": filtro_marcas,
    }
    
    full_params_for_logging = dict(await request.form())

    # Llamamos a la funciÃ³n manejadora central
    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id,
        report_key="ReportePuntosAlertaStock",
        processing_function=process_csv_puntos_alerta_stock, # Pasamos la funciÃ³n de lÃ³gica como argumento
        processing_params=processing_params,
        output_filename="ReportePuntosAlertaStock.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=log_session_id
    )


@app.post("/lista-basica-reposicion-historico", summary="RecomendaciÃ³n de Lista bÃ¡sica de reposiciÃ³n en funcion del histÃ³rico de ventas", tags=["AnÃ¡lisis"])
async def lista_basica_reposicion_historico(
    # Inyectamos el request para el logging
    request: Request, 
    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: str = Header(..., alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),

    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...),

    # --- ParÃ¡metros BÃ¡sicos ---
    ordenar_por: str = Form("Importancia", description="Criterio para ordenar el reporte final."),
    incluir_solo_categorias: str = Form("", description="String de categorÃ­as separadas por comas."),
    incluir_solo_marcas: str = Form("", description="String de marcas separadas por comas."),

    # --- ParÃ¡metros Avanzados ---
    # dias_analisis_ventas_recientes=30,
    # dias_analisis_ventas_general=180,
    dias_analisis_ventas_recientes: Optional[int] = Form(None, description="Ventana principal de anÃ¡lisis en dÃ­as. Ej: 30, 60, 90."),
    dias_analisis_ventas_general: Optional[int] = Form(None, description="Ventana secundaria de anÃ¡lisis para productos sin ventas recientes."),

    excluir_sin_ventas: str = Form("true", description="String 'true' o 'false' para excluir productos sin ventas."),
    # Usamos float e int para que FastAPI convierta los tipos automÃ¡ticamente
    lead_time_dias: float = Form(7.0),
    dias_cobertura_ideal_base: int = Form(10),
    peso_ventas_historicas: float = Form(0.6),
    # pesos_importancia_json: Optional[str] = Form(None, description='(Avanzado) Redefine los pesos del Ãndice de Importancia. Formato JSON.')
    # --- NUEVO: Recibimos los scores de la estrategia desde el frontend ---
    score_ventas: int = Form(...),
    score_ingreso: int = Form(...),
    score_margen: int = Form(...),
    score_dias_venta: int = Form(...),

    filtro_skus_json: Optional[str] = Form(None)
):
    user_id = None
    
    # --- LÃ“GICA DE DETERMINACIÃ“N DE CONTEXTO ---
    if current_user:
        # CASO 1: Usuario Registrado
        if not workspace_id:
            raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")
        user_id = current_user['email']
        print(f"PeticiÃ³n de usuario registrado: {user_id} para workspace: {workspace_id}")
        # Para el logging y descarga, pasamos el contexto del usuario
        log_session_id = None # No usamos el ID de sesiÃ³n anÃ³nima
    
    elif X_Session_ID:
        # CASO 2: Usuario AnÃ³nimo
        print(f"PeticiÃ³n de usuario anÃ³nimo: {X_Session_ID}")
        log_session_id = X_Session_ID
    else:
        # CASO 3: No hay identificador, denegamos el acceso
        raise HTTPException(status_code=401, detail="No se proporcionÃ³ autenticaciÃ³n ni ID de sesiÃ³n.")


    try:
        excluir_bool = excluir_sin_ventas.lower() == 'true'
        
        # Parseamos los strings JSON para convertirlos en listas de Python
        categorias_list = json.loads(incluir_solo_categorias) if incluir_solo_categorias else None
        marcas_list = json.loads(incluir_solo_marcas) if incluir_solo_marcas else None
        # pesos_importancia = json.loads(pesos_importancia_json) if pesos_importancia_json else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtros (categorÃ­as/marcas) invÃ¡lido. Se esperaba un string JSON.")

    # pesos_importancia = json.loads(pesos_importancia_json) if pesos_importancia_json else None
    # 1. Sumamos los scores recibidos
    total_scores = score_ventas + score_ingreso + score_margen + score_dias_venta

    # 2. Calculamos los pesos prorrateados, manejando el caso de divisiÃ³n por cero
    if total_scores == 0:
        # Si todos los scores son 0, asignamos un peso equitativo
        pesos_calculados = {'ventas': 0.25, 'ingreso': 0.25, 'margen': 0.25, 'dias_venta': 0.25}
    else:
        pesos_calculados = {
            'ventas': score_ventas / total_scores,
            'ingreso': score_ingreso / total_scores,
            'margen': score_margen / total_scores,
            'dias_venta': score_dias_venta / total_scores
        }

    try:
        filtro_skus = json.loads(filtro_skus_json) if filtro_skus_json else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro de SKUs invÃ¡lido.")


    # Preparamos el diccionario de parÃ¡metros para la funciÃ³n de lÃ³gica
    processing_params = {
        "filtro_skus": filtro_skus,
        "dias_analisis_ventas_recientes": dias_analisis_ventas_recientes,
        "dias_analisis_ventas_general": dias_analisis_ventas_general,
        "ordenar_por": ordenar_por,
        "incluir_solo_categorias": categorias_list,
        "incluir_solo_marcas": marcas_list,
        "excluir_sin_ventas": excluir_bool,
        "lead_time_dias": lead_time_dias,
        "dias_cobertura_ideal_base": dias_cobertura_ideal_base,
        "peso_ventas_historicas": peso_ventas_historicas,
        "pesos_importancia": pesos_calculados,
    }

    full_params_for_logging = dict(await request.form())

    # Llamamos a la funciÃ³n manejadora central
    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id,
        report_key="ReporteListaBasicaReposicionHistorica",
        processing_function=process_csv_lista_basica_reposicion_historico, # Pasamos la funciÃ³n de lÃ³gica como argumento
        processing_params=processing_params,
        output_filename="ReporteListaBasicaReposicionHistorica.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=log_session_id
    )



# ===================================================================================
# --- OTROS ENDPOINTS DE PRUEBA (EJEMPLO) ---
# ===================================================================================
# Por ahora, mantenemos estos endpoints
# ya que lo refactorizaremos en el siguiente paso para usar el nuevo flujo.

@app.post("/reporte-stock-minimo-sugerido", summary="RecomendaciÃ³n Stock de Alerta Ã³ MÃ­nimo Sugerido", tags=["AnÃ¡lisis"])
async def reporte_stock_minimo_sugerido(
    ventas: UploadFile = File(..., description="Archivo CSV con datos de ventas."),
    inventario: UploadFile = File(..., description="Archivo CSV con datos de inventario."),
    dias_cobertura_deseados: int = Form(...),
    meses_analisis_historicos: int = Form(...)
):
    """
    Sube archivos CSV de ventas e inventario, y realiza un anÃ¡lisis ABC
    segÃºn los criterios y perÃ­odo especificados.
    """

    # --- Leer archivo de ventas ---
    ventas_contents = await ventas.read()
    try:
        df_ventas = pd.read_csv(io.BytesIO(ventas_contents))
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error al leer el archivo CSV de ventas: {e}. Verifique el formato.")

    # --- Leer archivo de inventario ---
    inventario_contents = await inventario.read()
    try:
        df_inventario = pd.read_csv(io.BytesIO(inventario_contents))
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error al leer el archivo CSV de inventario: {e}. Verifique el formato.")

    # --- Procesamiento de los datos ---
    try:
        processed_df = process_csv_puntos_alerta_stock(
            df_ventas,
            df_inventario,
            # ParÃ¡metros de periodos para anÃ¡lisis de ventas
            # dias_analisis_ventas_recientes=30, #(P/FRONTEND) Anteriormente dias_importancia
            # dias_analisis_ventas_general=240,   #(P/FRONTEND) Anteriormente dias_promedio
            peso_ventas_historicas=0.6, # 0.0 = 100% reciente; 1.0 = 100% histÃ³rico
            # ParÃ¡metros para cÃ¡lculo de Stock Ideal
            dias_cobertura_ideal_base=10, #(P/FRONTEND) DÃ­as base para cobertura ideal
            coef_importancia_para_cobertura_ideal=0.05, # e.g., 0.25 (0 a 1), aumenta dÃ­as de cobertura ideal por importancia
            coef_rotacion_para_stock_ideal=0.10,       # e.g., 0.2 (0 a 1), aumenta stock ideal por rotaciÃ³n
            coef_rotacion_para_stock_minimo=0.15,
            # ParÃ¡metros para Pedido MÃ­nimo
            dias_cubrir_con_pedido_minimo=5, #(P/FRONTEND) DÃ­as de venta que un pedido mÃ­nimo deberÃ­a cubrir
            coef_importancia_para_pedido_minimo=0.1, # e.g., 0.5 (0 a 1), escala el pedido mÃ­nimo por importancia
            # Otros parÃ¡metros de comportamiento
            importancia_minima_para_redondeo_a_1=0.1, # e.g. 0.1, umbral de importancia para redondear pedidos pequeÃ±os a 1
            incluir_productos_pasivos=True,
            cantidad_reposicion_para_pasivos=1, # e.g., 1 o 2, cantidad a reponer para productos pasivos
            excluir_productos_sin_sugerencia_ideal=False, # Filtro para el resultado final
            # --- NUEVOS PARÃMETROS PARA EL PUNTO DE ALERTA ---
            lead_time_dias=10.0,
            dias_seguridad_base=0,
            factor_importancia_seguridad=1.0
        )
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=f"Error de validaciÃ³n: {str(ve)}")
    except Exception as e:
        # En un entorno de producciÃ³n, se deberÃ­a loggear este error de forma mÃ¡s robusta.
        print(f"Error interno durante el procesamiento: {e}") # Log para debugging
        raise HTTPException(status_code=500, detail=f"Error interno al procesar los datos. Por favor, contacte al administrador. Detalles: {str(e)}")

    # --- Manejo de DataFrame vacÃ­o ---
    if processed_df.empty:
        empty_excel = to_excel_with_autofit(pd.DataFrame(), sheet_name='Sin Datos')
        return StreamingResponse(
            empty_excel,
            media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            headers={
                "Content-Disposition": f"attachment; filename=stock_minimo_sugerido_{datetime.now().strftime('%Y%m%d')}.xlsx",
                "X-Status-Message": "No se encontraron datos para los criterios o perÃ­odo seleccionados."
            }
        )

    # --- Exportar a Excel ---
    try:
        excel_output = to_excel_with_autofit(processed_df, sheet_name='Analisis_ABC')
    except Exception as e:
        print(f"Error al generar el archivo Excel: {e}") # Log para debugging
        raise HTTPException(status_code=500, detail=f"Error al generar el archivo Excel: {str(e)}")

    # filename_period = "todo_historial" if periodo_abc == 0 else f"ultimos_{periodo_abc}_meses"
    final_filename = f"stock_minimo_sugerido_{datetime.now().strftime('%Y%m%d')}.xlsx"

    return StreamingResponse(
        excel_output,
        media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        headers={"Content-Disposition": f"attachment; filename={final_filename}"}
    )







@app.post("/reposicion-stock")
async def upload_csvs(
    ventas: UploadFile = File(...),
    inventario: UploadFile = File(...)
):
    # Leer archivo de ventas
    ventas_contents = await ventas.read()
    df_ventas = pd.read_csv(io.BytesIO(ventas_contents))

    # Leer archivo de inventario
    inventario_contents = await inventario.read()
    df_inventario = pd.read_csv(io.BytesIO(inventario_contents))

    # Procesamiento conjunto
    # processed_df = process_csv_reponer_stock(df_ventas, df_inventario)
    processed_df = process_csv_reponer_stock(
        df_ventas,
        df_inventario,
        # ParÃ¡metros de periodos para anÃ¡lisis de ventas
        dias_analisis_ventas_recientes=30, #(P/FRONTEND) Anteriormente dias_importancia
        dias_analisis_ventas_general=240,   #(P/FRONTEND) Anteriormente dias_promedio
        # ParÃ¡metros para cÃ¡lculo de Stock Ideal
        dias_cobertura_ideal_base=10, #(P/FRONTEND) DÃ­as base para cobertura ideal
        coef_importancia_para_cobertura_ideal=0.25, # e.g., 0.25 (0 a 1), aumenta dÃ­as de cobertura ideal por importancia
        coef_rotacion_para_stock_ideal=0.20,       # e.g., 0.2 (0 a 1), aumenta stock ideal por rotaciÃ³n
        # ParÃ¡metros para Pedido MÃ­nimo
        dias_cubrir_con_pedido_minimo=3, #(P/FRONTEND) DÃ­as de venta que un pedido mÃ­nimo deberÃ­a cubrir
        coef_importancia_para_pedido_minimo=0.5, # e.g., 0.5 (0 a 1), escala el pedido mÃ­nimo por importancia
        # Otros parÃ¡metros de comportamiento
        importancia_minima_para_redondeo_a_1=0.1, # e.g. 0.1, umbral de importancia para redondear pedidos pequeÃ±os a 1
        incluir_productos_pasivos=True,
        cantidad_reposicion_para_pasivos=1, # e.g., 1 o 2, cantidad a reponer para productos pasivos
        excluir_productos_sin_sugerencia_ideal=True # Filtro para el resultado final
    )


    # output = io.BytesIO()
    # processed_df.to_excel(output, index=False, engine='openpyxl')
    # output.seek(0)

    # return StreamingResponse(output, media_type="text/csv", headers={"Content-Disposition": "attachment; filename=analisis_abc.xls"})
    return StreamingResponse(
        # output,
        to_excel_with_autofit(processed_df, sheet_name='Hoja 1'),
        media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        headers={
            "Content-Disposition": "attachment; filename=reposicion-stock.xlsx"
        }
    )


# --- 4. CREA EL NUEVO ENDPOINT ---
@app.post("/auditoria-margenes", summary="Genera el reporte de AuditorÃ­a de MÃ¡rgenes", tags=["Reportes"])
async def generar_auditoria_margenes(
    # Inyectamos el request para el logging
    request: Request, 
    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: str = Header(..., alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),

    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...),
    incluir_solo_categorias: str = Form("", description="String de categorÃ­as separadas por comas."),
    incluir_solo_marcas: str = Form("", description="String de marcas separadas por comas."),

    periodo_analisis_dias: int = Form(30),

    # --- Recibimos los nuevos parÃ¡metros del formulario ---
    tipo_analisis_margen: str = Form("desviacion_negativa"),
    umbral_desviacion_porcentaje: float = Form(10.0),
    ordenar_por: str = Form("impacto_financiero")
):
    
    user_id = current_user['email'] if current_user else None
    if user_id and not workspace_id:
        raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")

    try:
        filtro_categorias = json.loads(incluir_solo_categorias) if incluir_solo_categorias else None
        filtro_marcas = json.loads(incluir_solo_marcas) if incluir_solo_marcas else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro invÃ¡lido.")


    processing_params = {
        "tipo_analisis_margen": tipo_analisis_margen,
        "umbral_desviacion_porcentaje": umbral_desviacion_porcentaje,
        "filtro_categorias": filtro_categorias,
        "filtro_marcas": filtro_marcas,
        "periodo_analisis_dias": periodo_analisis_dias,
        "ordenar_por": ordenar_por
    }
    
    full_params_for_logging = dict(await request.form())
    
    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        report_key="ReporteAuditoriaMargenes",
        processing_function=auditar_margenes_de_productos_nuevo, # La nueva funciÃ³n de lÃ³gica
        processing_params=processing_params,
        output_filename="Auditoria_Margenes.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=X_Session_ID,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id
    )


@app.post("/diagnostico-catalogo", summary="Genera el DiagnÃ³stico de CatÃ¡logo", tags=["Reportes"])
async def generar_diagnostico_catalogo(
    request: Request,
    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: str = Header(..., alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),

    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...),
    # --- Recibimos los nuevos parÃ¡metros del formulario ---
    tipo_diagnostico_catalogo: str = Form(...),
    filtro_stock: str = Form("todos"),
    dias_inactividad: int = Form(365),
    ordenar_por: str = Form("valor_stock_s"),
    incluir_solo_categorias: Optional[str] = Form(None),
    incluir_solo_marcas: Optional[str] = Form(None)
):
    user_id = current_user['email'] if current_user else None
    if user_id and not workspace_id:
        raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")

    try:
        filtro_categorias = json.loads(incluir_solo_categorias) if incluir_solo_categorias else None
        filtro_marcas = json.loads(incluir_solo_marcas) if incluir_solo_marcas else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro invÃ¡lido.")

    processing_params = {
        "tipo_diagnostico_catalogo": tipo_diagnostico_catalogo,
        "filtro_stock": filtro_stock,
        "dias_inactividad": dias_inactividad,
        "filtro_categorias": filtro_categorias,
        "filtro_marcas": filtro_marcas,
        "ordenar_por": ordenar_por
    }

    full_params_for_logging = dict(await request.form())
    
    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        report_key="ReporteDiagnosticoCatalogo",
        processing_function=diagnosticar_catalogo, # La nueva funciÃ³n de lÃ³gica
        processing_params=processing_params,
        # ... (el resto de los argumentos para el manejador)
        output_filename="Diagnostico_Catalogo.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=X_Session_ID,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id
    )


@app.post("/auditoria-calidad-datos", summary="Genera la AuditorÃ­a de Calidad de Datos", tags=["Reportes"])
async def generar_auditoria_calidad_datos(
    request: Request,
    # Definimos explÃ­citamente los parÃ¡metros que la LÃ“GICA necesit
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: str = Header(..., alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),

    inventario_file_id: str = Form(...), # Este reporte solo necesita el inventario
    # --- Recibimos los nuevos parÃ¡metros del formulario ---
    criterios_auditoria_json: str = Form(...),
    incluir_solo_categorias: Optional[str] = Form(None),
    incluir_solo_marcas: Optional[str] = Form(None),
    ordenar_por: str = Form("valor_stock_s"),
    filtro_skus_json: Optional[str] = Form(None)    
):
    user_id = current_user['email'] if current_user else None
    if user_id and not workspace_id:
        raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")

    try:
        criterios_auditoria = json.loads(criterios_auditoria_json)
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de criterios de auditorÃ­a invÃ¡lido.")

    try:
        filtro_categorias = json.loads(incluir_solo_categorias) if incluir_solo_categorias else None
        filtro_marcas = json.loads(incluir_solo_marcas) if incluir_solo_marcas else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro invÃ¡lido.")

    try:
        filtro_skus = json.loads(filtro_skus_json) if filtro_skus_json else None
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Formato de filtro de SKUs invÃ¡lido.")


    processing_params = {
        "filtro_skus": filtro_skus,
        "criterios_auditoria": criterios_auditoria,
        "filtro_categorias": filtro_categorias,
        "ordenar_por": ordenar_por,
        "filtro_marcas": filtro_marcas
    }
    
    full_params_for_logging = dict(await request.form())

    return await _handle_report_generation(
        full_params_for_logging=full_params_for_logging,
        report_key="ReporteAuditoriaCalidadDatos",
        processing_function=auditar_calidad_datos, # La nueva funciÃ³n de lÃ³gica
        processing_params=processing_params,
        inventario_file_id=inventario_file_id,
        # Pasamos None para el archivo de ventas, ya que no se necesita
        ventas_file_id=None, 
        output_filename="Auditoria_Calidad_de_Datos.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=X_Session_ID,
    )


# ----------------------------------------------------------
# ------------------ FUNCIONES AUXILIARES ------------------
# ----------------------------------------------------------
async def _handle_report_generation(
    # user_id: Optional[str] = None, # <-- ParÃ¡metro para el futuro (usuarios registrados)
    # workspace_id: Optional[str] = None,
    # session_id: Optional[str] = None
    # Ya no depende de 'request'. Recibe todos los contextos necesarios.
    full_params_for_logging: Dict[str, Any],
    report_key: str,
    processing_function: Callable,
    processing_params: Dict[str, Any],
    output_filename: str,
    # ParÃ¡metros de contexto. Solo un conjunto estarÃ¡ presente.
    user_id: Optional[str],
    workspace_id: Optional[str],
    session_id: Optional[str],
    # IDs de los archivos a procesar
    ventas_file_id: Optional[str],
    inventario_file_id: Optional[str],
    is_unlimited_anonymous: bool = False
):
    """
    FunciÃ³n central refactorizada que maneja la generaciÃ³n de CUALQUIER reporte
    para usuarios anÃ³nimos O registrados, aplicando la lÃ³gica de negocio correcta.
    """
    # --- PASO 1: DETERMINAR EL CONTEXTO Y LA REFERENCIA BASE EN FIRESTORE ---
    report_config = REPORTS_CONFIG.get(report_key)
    if not report_config:
        raise HTTPException(status_code=404, detail=f"La configuraciÃ³n para el reporte '{report_key}' no fue encontrada.")
    
    report_cost = report_config['costo']
    is_pro_report = report_config['isPro']

    # Esta es la lÃ³gica clave. `entity_ref` apuntarÃ¡ al documento que contiene los crÃ©ditos.
    if user_id:
        # CONTEXTO: Usuario Registrado
        print(f"Procesando para usuario registrado: {user_id}")
        entity_ref = db.collection('usuarios').document(user_id)
    elif session_id:
        # CONTEXTO: SesiÃ³n AnÃ³nima
        print(f"Procesando para sesiÃ³n anÃ³nima: {session_id}")
        entity_ref = db.collection('sesiones_anonimas').document(session_id)
    else:
        # Si no hay ningÃºn identificador, es un error de lÃ³gica interna.
        raise HTTPException(status_code=500, detail="Error interno: no se pudo determinar el contexto de la sesiÃ³n.")

    # --- PASO 2: VALIDACIONES DE NEGOCIO (Usando la referencia correcta) ---
    
    # GuardiÃ¡n de Acceso Pro
    if is_pro_report and user_id is None:
        print(f"ðŸš« Acceso denegado: SesiÃ³n anÃ³nima ({session_id}) intentÃ³ acceder al reporte PRO '{report_key}'.")
        raise HTTPException(status_code=403, detail="Este es un reporte 'Pro'. Debes registrarte para acceder.")

    # Cajero (VerificaciÃ³n de CrÃ©ditos)
    entity_doc = entity_ref.get()
    if not entity_doc.exists:
        raise HTTPException(status_code=404, detail="La sesiÃ³n o el perfil de usuario no existe.")
    
    creditos_restantes = entity_doc.to_dict().get("creditos_restantes", 0)
    if creditos_restantes < report_cost:
        raise HTTPException(status_code=402, detail=f"CrÃ©ditos insuficientes. Este reporte requiere {report_cost} crÃ©ditos y solo tienes {creditos_restantes}.")

    # --- PASO 3: PROCESAMIENTO Y GENERACIÃ“N (DENTRO DE UN TRY/EXCEPT) ---
    # Si algo falla aquÃ­, es un error de ejecuciÃ³n. Lo registraremos como "fallido" sin cobrar.
    try:
        # --- INICIO DE LA NUEVA LÃ“GICA DE CARGA CONDICIONAL ---
        print("Iniciando carga de datos condicional...")
        
        tasks = []
        # Creamos una lista de tareas de descarga solo para los archivos que existen
        if ventas_file_id:
            tasks.append(descargar_contenido_de_storage(user_id, workspace_id, session_id, ventas_file_id))
        if inventario_file_id:
            tasks.append(descargar_contenido_de_storage(user_id, workspace_id, session_id, inventario_file_id))

        # Ejecutamos las tareas que se aÃ±adieron
        results = await asyncio.gather(*tasks)

        # Asignamos los resultados de vuelta con cuidado
        df_ventas = pd.DataFrame() # Default: DataFrame vacÃ­o
        df_inventario = pd.DataFrame() # Default: DataFrame vacÃ­o
        
        result_index = 0
        if ventas_file_id:
            df_ventas = pd.read_csv(io.BytesIO(results[result_index]))
            result_index += 1
        if inventario_file_id:
            df_inventario = pd.read_csv(io.BytesIO(results[result_index]))

        print("âœ… Datos cargados y convertidos a DataFrames exitosamente.")
        # --- FIN DE LA NUEVA LÃ“GICA DE CARGA ---


        # Ahora, pasamos los DataFrames ya cargados a la funciÃ³n de lÃ³gica
        processing_result = processing_function(
            df_ventas=df_ventas.copy(), 
            df_inventario=df_inventario.copy(), 
            **processing_params
        )
        
        # Extraemos las partes del resultado
        resultado_df = processing_result.get("data")
        summary_data = processing_result.get("summary")

        # --- INICIO DE LA NUEVA LÃ“GICA DE TRUNCADO ---
        is_truncated = False
        total_rows = 0

        # if user_id is None: # Si es un usuario anÃ³nimo
        if user_id is None and not is_unlimited_anonymous: 
            if not resultado_df.empty:
                total_rows = len(resultado_df)
                if total_rows > 15:
                    print(f"Truncando resultado para sesiÃ³n anÃ³nima. Mostrando 15 de {total_rows} filas.")
                    resultado_df = resultado_df.head(15)
                    is_truncated = True
        
        columnas = resultado_df.columns
        columnas_duplicadas = columnas[columnas.duplicated()].unique().tolist()
        
        if resultado_df is None or summary_data is None:
            raise ValueError("La funciÃ³n de procesamiento no devolviÃ³ la estructura de datos esperada.")

        updated_credits = None

        # Verificamos si el DataFrame resultante estÃ¡ vacÃ­o
        if resultado_df.empty:
            print(f"âš ï¸ Reporte '{report_key}' generado pero sin resultados. No se cobrarÃ¡n crÃ©ditos.")
            
            # Registramos el evento con costo 0 y un estado claro.
            log_report_generation(
                user_id=user_id, workspace_id=workspace_id, session_id=session_id,
                report_name=report_key, params=full_params_for_logging,
                ventas_file_id=ventas_file_id, inventario_file_id=inventario_file_id,
                creditos_consumidos=0, estado="exitoso_vacio"
            )

            # Devolvemos una respuesta JSON exitosa (200 OK) pero con datos vacÃ­os
            # y un insight que explica la situaciÃ³n.
            return JSONResponse(content={
                "insight": "No se encontraron productos que coincidan con los parÃ¡metros seleccionados.",
                "kpis": {}, # Devolvemos un objeto de KPIs vacÃ­o
                "data": [],  # Devolvemos una lista de datos vacÃ­a
                "report_key": report_key,
                "is_truncated": is_truncated, # <-- Nuevo flag
                "total_rows": total_rows  
            })

        # if columnas_duplicadas:
        #     print("\n--- ðŸ•µï¸  DEBUG: Â¡ADVERTENCIA DE COLUMNAS DUPLICADAS! ---")
        #     print(f"El DataFrame final para el reporte '{report_key}' tiene nombres de columna repetidos:")
        #     print(f"Columnas duplicadas encontradas: {columnas_duplicadas}")
        #     print("Esto causarÃ¡ que se omitan datos en el frontend. Revisa tu funciÃ³n de procesamiento para renombrar o eliminar estas columnas antes de devolver el DataFrame.")
        #     print("-----------------------------------------------------------\n")
        #     # Opcional: PodrÃ­as decidir lanzar un error aquÃ­ para forzar la correcciÃ³n
        #     # raise ValueError(f"Columnas duplicadas detectadas: {columnas_duplicadas}")
        # # --- FIN DEL BLOQUE DE DEPURACIÃ“N ---
        
        # --- INICIO DE LA NUEVA LÃ“GICA DE LIMPIEZA CENTRALIZADA ---
        print("Ejecutando limpieza de datos centralizada...")

        # 1. Reemplazamos infinitos con NaN
        df_limpio = resultado_df.replace([np.inf, -np.inf], np.nan)
        
        # 2. Convertimos el DataFrame limpio a un diccionario. 
        #    Este paso puede convertir los pd.NA o NaT a NaN de Python.
        records = df_limpio.to_dict(orient='records')

        # 3. Iteramos sobre la lista de diccionarios para reemplazar los NaN por None.
        #    Esta es la forma mÃ¡s segura de garantizar la compatibilidad con JSON.
        data_for_frontend = [
            {k: (None if pd.isna(v) else v) for k, v in row.items()}
            for row in records
        ]
        print("âœ… Limpieza de datos completada.")

        insight_text = f"AnÃ¡lisis completado. Se encontraron {len(resultado_df)} productos que cumplen los criterios."
        if resultado_df.empty:
            insight_text = "El anÃ¡lisis se completÃ³, pero no se encontraron productos con los filtros seleccionados."
        
       
        # Usamos la `entity_ref` correcta para descontar crÃ©ditos
        entity_ref.update({"creditos_restantes": firestore.Increment(-report_cost)})
            
        # --- CAMBIO CLAVE: Leemos el nuevo saldo DESPUÃ‰S de la actualizaciÃ³n ---
        updated_doc = entity_ref.get()
        updated_data = updated_doc.to_dict()
        new_remaining = updated_data.get("creditos_restantes", 0)
        initial_credits = updated_data.get("creditos_iniciales", 0)
        
        updated_credits = {
            "used": initial_credits - new_remaining,
            "remaining": new_remaining
        }

        log_report_generation(
            user_id=user_id, workspace_id=workspace_id, session_id=session_id,
            report_name=report_key, params=full_params_for_logging,
            ventas_file_id=ventas_file_id, inventario_file_id=inventario_file_id,
            creditos_consumidos=report_cost, estado="exitoso"
        )

        if user_id and workspace_id:
            workspace_ref = db.collection('usuarios').document(user_id).collection('espacios_trabajo').document(workspace_id)
            workspace_ref.update({"fechaModificacion": datetime.now(timezone.utc)})

        return JSONResponse(content={
            "insight": summary_data.get("insight"),
            "kpis": summary_data.get("kpis"),
            "data": data_for_frontend,
            "report_key": report_key,
            "updated_credits": updated_credits,
            "is_truncated": is_truncated, # <-- Nuevo flag
            "total_rows": total_rows  
        })

    except Exception as e:
        # Si CUALQUIER COSA falla durante el procesamiento...
        # --- BLOQUE DE MANEJO DE ERRORES MEJORADO ---
        print("\n" + "="*50)
        print("ðŸ”¥ðŸ”¥ðŸ”¥ OCURRIÃ“ UN ERROR CRÃTICO DURANTE EL PROCESAMIENTO ðŸ”¥ðŸ”¥ðŸ”¥")
        
        # Esta lÃ­nea imprimirÃ¡ el traceback completo, diciÃ©ndonos el archivo,
        # la lÃ­nea y el tipo de error exacto.
        traceback.print_exc()
        
        print("="*50 + "\n")
        # ... registramos el intento fallido SIN descontar crÃ©ditos.
        user_message, error_type, tech_details = "Error inesperado al procesar", type(e).__name__, str(e)
        if isinstance(e, KeyError): user_message = f"Columna requerida no encontrada: {e}"

        log_report_generation(
            user_id=user_id, workspace_id=workspace_id, session_id=session_id,
            report_name=report_key, params=full_params_for_logging,
            ventas_file_id=ventas_file_id, inventario_file_id=inventario_file_id,
            creditos_consumidos=0, estado="fallido",
            error_details={"user_message": user_message, "error_type": type(e).__name__, "technical_details": str(e)}
        )

        if user_id and workspace_id:
            workspace_ref = db.collection('usuarios').document(user_id).collection('espacios_trabajo').document(workspace_id)
            workspace_ref.update({"fechaModificacion": datetime.now(timezone.utc)})

        raise HTTPException(status_code=500, detail=user_message)


def to_excel_with_autofit(df, sheet_name='Sheet1'):
    """
    Writes a pandas DataFrame to an Excel file in BytesIO, 
    autofitting column widths.

    Args:
        df (pd.DataFrame): The DataFrame to write.
        sheet_name (str, optional): Name of the sheet. Defaults to 'Sheet1'.

    Returns:
         io.BytesIO: In-memory Excel file.
    """
    output = io.BytesIO()
    writer = pd.ExcelWriter(output, engine='xlsxwriter')
    df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=1, header=False)

    workbook = writer.book
    worksheet = writer.sheets[sheet_name]

    worksheet.freeze_panes(1, 2)

    # worksheet.set_row(0, 50, None, {'align':'vcenter', 'center_across':True, 'bold': True})

    # Add a header format.
    header_format = workbook.add_format({
        'bold': True,
        'text_wrap': True,
        'valign': 'vcenter',
        'align': 'center',
        'font_color': 'black',
        'fg_color': '#E5E0EC',
        'border_color': 'white',
        'border': 1})

    # Write the column headers with the defined format.
    for col_num, value in enumerate(df.columns.values):
        worksheet.write(0, col_num, value, header_format)

    worksheet.set_row(0, 52)

    worksheet.autofilter(0, 0, df.shape[0], df.shape[1])

    for i, column in enumerate(df.columns):
        # column_width = max(df[column].astype(str).map(len).max(), len(column))
        column_width = 13
        if (i == 0):
            column_width = 8
        if (i == 1):
            column_width = 50
        if (i == 2):
            column_width = 24
        if (i == 3):
            column_width = 24
        worksheet.set_column(i, i, column_width + 2)

    # Hide columns based on sheet_name
    if (sheet_name == 'Puntos_Alerta_Stock'):
        worksheet.set_column('C:C', 26, None, {'hidden': True})
        worksheet.set_column('D:D', 26, None, {'hidden': True})
        worksheet.set_column('E:E', 15, None, {'hidden': True})


    writer.close()
    output.seek(0)
    return output


# ===================================================
# ============== FINAL: FULL REPORTES ===============
# ===================================================
# ===================================================
# ===================================================
# ===================================================

# ===================================================================================
# --- ENDPOINT DE DIAGNÃ“STICO TEMPORAL ---
# ===================================================================================
@app.post("/debug/auditoria-margenes", summary="[Debug] Audita los mÃ¡rgenes para encontrar inconsistencias", tags=["Debug"])
async def run_auditoria_margenes(
    request: Request,
    # Recibe los mismos parÃ¡metros de contexto que tus otros reportes
    current_user: Optional[dict] = Depends(get_current_user_optional),
    X_Session_ID: Optional[str] = Header(None, alias="X-Session-ID"),
    workspace_id: Optional[str] = Form(None),
    ventas_file_id: str = Form(...),
    inventario_file_id: str = Form(...)
):
    """
    Este endpoint especial ejecuta la auditorÃ­a de mÃ¡rgenes y devuelve un
    reporte con los productos que se estÃ¡n vendiendo por debajo del costo.
    """
    # Determinamos el contexto (anÃ³nimo o registrado)
    user_id = current_user['email'] if current_user else None
    if user_id and not workspace_id:
        raise HTTPException(status_code=400, detail="Se requiere un 'workspace_id' para usuarios autenticados.")

    print(f"ðŸ”¥ current_user: {user_id}")
    print(f"ðŸ”¥ X_Session_ID: {X_Session_ID}")
    print(f"ðŸ”¥ workspace_id: {workspace_id}")
    print(f"ðŸ”¥ ventas_file_id: {ventas_file_id}")
    print(f"ðŸ”¥ inventario_file_id: {inventario_file_id}")

    # Llamamos a nuestra funciÃ³n manejadora central, pasÃ¡ndole la nueva funciÃ³n de auditorÃ­a
    return await _handle_report_generation(
        full_params_for_logging=dict(await request.form()),
        report_key="AuditoriaMargenes", # Una clave interna para este reporte
        processing_function=auditar_margenes_de_productos,
        processing_params={}, # La funciÃ³n de auditorÃ­a no necesita parÃ¡metros extra
        output_filename="Auditoria_De_Margenes.xlsx",
        user_id=user_id,
        workspace_id=workspace_id,
        session_id=X_Session_ID,
        ventas_file_id=ventas_file_id,
        inventario_file_id=inventario_file_id
    )



# ===================================================================================
# --- ENDPOINT PARA BETA TESTERS ---
# ===================================================================================
class BetaRegistrationPayload(BaseModel):
    perfil: str
    desafios: List[str]
    desafio_otro: Optional[str] = ""
    nombre: str
    email: str
    whatsapp: str
    nombre_negocio: str
    ciudad_pais: str


# --- Endpoint de Registro ---
@app.post("/beta/register", summary="Registra un nuevo usuario para la beta", tags=["Beta"])
async def register_for_beta(payload: BetaRegistrationPayload, request: Request):
    """
    Recibe los datos del formulario de onboarding, obtiene la geolocalizaciÃ³n del usuario
    y guarda toda la informaciÃ³n en una colecciÃ³n de Firestore.
    """
    # 1. Obtener datos de geolocalizaciÃ³n basados en la IP del cliente
    client_ip = request.client.host
    geoloc_data = {"ip": client_ip, "status": "desconocido"}
    if client_ip and client_ip not in ["127.0.0.1", "testclient"]:
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"http://ip-api.com/json/{client_ip}")
                response.raise_for_status()
                api_data = response.json()
                if api_data.get("status") == "success":
                    geoloc_data = {
                        "ip": api_data.get("query"),
                        "pais": api_data.get("country"),
                        "ciudad": api_data.get("city"),
                        "region": api_data.get("regionName"),
                        "isp": api_data.get("isp"),
                        "status": "exitoso"
                    }
        except httpx.RequestError as e:
            # Si la API de geolocalizaciÃ³n falla, no detenemos el proceso, solo lo registramos.
            print(f"ðŸ”¥ Advertencia: No se pudo geolocalizar al usuario {payload.email}. Error: {e}")

    # 2. Preparar los datos para guardar en Firestore
    try:
        # Convertir el payload de Pydantic a un diccionario
        user_data = payload.dict()

        now_iso = datetime.now(timezone.utc).isoformat()

        # AÃ±adir la informaciÃ³n adicional
        user_data["fecha_registro"] = now_iso
        user_data["geolocalizacion"] = geoloc_data
        
        # 3. Guardar en Firestore
        # Crea un nuevo documento en la colecciÃ³n 'beta_registrations'
        # Firestore generarÃ¡ un ID Ãºnico para el documento automÃ¡ticamente.
        doc_ref = db.collection('beta_registrations').document()
        doc_ref.set(user_data)

        return {
            "status": "success",
            "message": "Usuario registrado exitosamente en la beta.",
            "user_id": doc_ref.id # Devuelve el ID del nuevo documento creado
        }
        
    except Exception as e:
        # Manejo de errores en caso de que falle la escritura en Firestore
        raise HTTPException(
            status_code=500,
            detail=f"OcurriÃ³ un error al registrar al usuario: {e}"
        )


# ===================================================================================
# --- ENDPOINT PARA ANONIMOS NO LIMITADOS ---
# ===================================================================================
@app.post("/api/v1/anonymous-analysis", summary="Ejecuta un anÃ¡lisis para un usuario anÃ³nimo", tags=["AnÃ¡lisis AnÃ³nimo"])
async def run_anonymous_analysis(
    request: Request,
    ventas_file: UploadFile = File(...),
    inventario_file: UploadFile = File(...),
    report_type: str = Form(...)
):
    """
    Este endpoint es la puerta de entrada para nuevos usuarios.
    1. Valida si el usuario puede ejecutar un anÃ¡lisis (1 por 24h).
    2. Crea una sesiÃ³n anÃ³nima.
    3. Sube los archivos a esa sesiÃ³n.
    4. Llama al manejador de reportes en modo "ilimitado".
    5. Devuelve un contrato de datos especÃ­fico para el frontend.
    """
    client_ip = request.client.host
    now = datetime.now(timezone.utc)
    
    # --- PASO 1 (sin cambios) ---
    usage_logs_ref = db.collection('anonymous_usage_logs').document(client_ip)
    usage_doc = usage_logs_ref.get()
    
    if usage_doc.exists:
        last_usage_time = usage_doc.to_dict().get("timestamp")
        if now - last_usage_time < timedelta(hours=24):
            return JSONResponse(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                content={
                    "error": "ANONYMOUS_SESSION_LIMIT_EXCEEDED",
                    "message": "Ya has utilizado tu anÃ¡lisis anÃ³nimo gratuito por hoy. RegÃ­strate para continuar."
                }
            )

    # --- PASO 2 (sin cambios) ---
    session_id = str(uuid.uuid4())
    session_ref = db.collection('sesiones_anonimas').document(session_id)
    session_ref.set({
        "fechaCreacion": now,
        "ultimoAcceso": now,
        "creditos_iniciales": 18,
        "creditos_restantes": 18,
        "estrategia": DEFAULT_STRATEGY,
        "geolocalizacion": {"ip": client_ip}
    })
    
    # --- PASO 3: Subir Archivos a la Nueva SesiÃ³n (CORREGIDO) ---
    try:
        # FunciÃ³n auxiliar para encapsular el proceso de subir y registrar
        async def _upload_and_log_file(file: UploadFile, tipo: str, contents: bytes):
            timestamp_str = now.strftime('%Y-%m-%d_%H%M%S')
            
            # 1. Subir a Storage
            ruta_storage = upload_to_storage(
                user_id=None,
                workspace_id=None,
                session_id=session_id,
                file_contents=contents,
                tipo_archivo=tipo,
                original_filename=file.filename,
                content_type=file.content_type,
                timestamp_str=timestamp_str
            )
            
            # 2. Registrar en Firestore
            file_id = f"{timestamp_str}_{tipo}"
            metadata = extraer_metadatos_df(pd.read_csv(io.BytesIO(contents)), tipo)
            
            # --- CORRECCIÃ“N: Eliminamos el 'await' de una funciÃ³n sÃ­ncrona ---
            log_file_upload_in_firestore( # <-- AWAIT ELIMINADO
                user_id=None,
                workspace_id=None,
                session_id=session_id,
                file_id=file_id,
                tipo_archivo=tipo,
                nombre_original=file.filename,
                ruta_storage=ruta_storage,
                metadata=metadata,
                timestamp_obj=now
            )
            return file_id

        # El resto de la lÃ³gica de subida no cambia
        ventas_contents = await ventas_file.read()
        inventario_contents = await inventario_file.read()
        
        ventas_task = asyncio.create_task(_upload_and_log_file(ventas_file, 'ventas', ventas_contents))
        inventario_task = asyncio.create_task(_upload_and_log_file(inventario_file, 'inventario', inventario_contents))
        
        ventas_file_id, inventario_file_id = await asyncio.gather(ventas_task, inventario_task)
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=400, detail=f"Error al procesar los archivos CSV: {e}")

    # --- PASOS 4, 5 y 6 (sin cambios) ---
    report_config_found = next((v for k, v in REPORTS_CONFIG.items() if v.get("url_key") == report_type), None)
    
    if not report_config_found:
        raise HTTPException(status_code=404, detail=f"El tipo de reporte '{report_type}' no es vÃ¡lido.")

    report_key = report_config_found['key']
    processing_function = globals()[report_config_found['processing_function_name']]

    try:
        response_from_handler = await _handle_report_generation(
            full_params_for_logging={"report_type": report_type},
            report_key=report_key,
            processing_function=processing_function,
            processing_params=report_config_found.get("default_params", {}),
            output_filename="anonymous_report.xlsx",
            user_id=None,
            workspace_id=None,
            session_id=session_id,
            ventas_file_id=ventas_file_id,
            inventario_file_id=inventario_file_id,
            is_unlimited_anonymous=True
        )
        analysis_data = json.loads(response_from_handler.body)
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error interno al ejecutar el anÃ¡lisis: {e}")

    usage_logs_ref.set({"timestamp": now, "session_id": session_id})

    final_response = {
        "session_id": session_id,
        "analysis_result": {
            "report_key": analysis_data.get("report_key"),
            "data": analysis_data.get("data"),
            "kpis": analysis_data.get("kpis"),
            "insight": analysis_data.get("insight")
        },
        "credits_granted": 10
    }
    
    return JSONResponse(content=final_response)


